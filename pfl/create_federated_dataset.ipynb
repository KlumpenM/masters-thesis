{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creating a `pfl` Dataset Object\n",
    "A `pfl.data.Dataset` object is the basic representation for a single user's dataset. Alternatively, this can also be used for building a central dataset for evaluation purpose (to be used with `pfl.callback.CentralEvaluationCallback`).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.26082601 -1.12266244 -1.78670903 -1.67631867]\n",
      " [ 0.36388835  0.08802618  0.44563335  0.37226589]\n",
      " [ 1.64369741 -0.67493593  1.69465502  1.17460049]\n",
      " [ 0.03360219  0.71709835  0.25154238  0.36946786]\n",
      " [-1.20314452 -0.84598324 -1.6381041  -1.49313281]\n",
      " [ 1.4854961  -1.81824661  1.17298631  0.49046328]\n",
      " [-0.23414879  0.30232747 -0.18022215 -0.06987397]\n",
      " [-2.68763717  2.32659373 -2.40802671 -1.34256486]\n",
      " [-0.38952767  0.76608295 -0.22172902  0.00812763]\n",
      " [-1.05540026 -1.35143849 -1.61777354 -1.59778114]]\n",
      "Batch from `Dataset`: \n",
      "x shape=(5, 4), x=[[-1.26082601 -1.12266244 -1.78670903 -1.67631867]\n",
      " [ 0.36388835  0.08802618  0.44563335  0.37226589]\n",
      " [ 1.64369741 -0.67493593  1.69465502  1.17460049]\n",
      " [ 0.03360219  0.71709835  0.25154238  0.36946786]\n",
      " [-1.20314452 -0.84598324 -1.6381041  -1.49313281]]\n",
      "y shape=(5,), y=[0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets\n",
    "from pfl.data.dataset import Dataset\n",
    "\n",
    "# Create synthetic user data\n",
    "user_id = 'pfl-user-0'\n",
    "user_features, user_labels = sklearn.datasets.make_classification(n_samples=10, n_features=4)\n",
    "\n",
    "# Create a pfl Dataset object\n",
    "user_dataset = Dataset(raw_data=[user_features, user_labels], user_id=user_id)\n",
    "\n",
    "# Get a batch of the user dataset using Dataset.iter\n",
    "batch_x, batch_y = next(user_dataset.iter(batch_size=5))\n",
    "print(f\"Batch from `Dataset`: \\n\"\n",
    "      f\"x shape={batch_x.shape}, x={batch_x}\\n\"\n",
    "      f\"y shape={batch_y.shape}, y={batch_y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Creating a PyTorch `pfl` Dataset Object\n",
    "The above example showed how to create the `pfl` dataset in numpy format. `pfl`  supports data that are processed in PyTorch or TensorFlow tensors. We will focus on PyTorch for the purpose of this tutorial. The TensorFlow dataset implementation can be found at [tensorflow.py](https://github.com/apple/pfl-research/blob/main/pfl/data/tensorflow.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foo\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "os.environ['PFL_PYTORCH_DEVICE'] = 'cpu'\n",
    "\n",
    "import torch\n",
    "\n",
    "# Set multiprocessing start method to \"spawn\" instead of forkserver (which is the default)\n",
    "# That is because forkserver does not work on Windows, but spawn does.\n",
    "def init_multiprocessing():\n",
    "    try:\n",
    "        multiprocessing.set_start_method(\"spawn\", force=True)  # Forces \"spawn\"\n",
    "    except RuntimeError:\n",
    "        pass  # Ignore if it's already set\n",
    "\n",
    "init_multiprocessing()\n",
    "\n",
    "from pfl.internal.ops.selector import set_framework_module\n",
    "from pfl.internal.ops import pytorch_ops, numpy_ops\n",
    "from pfl.data.pytorch import PyTorchTensorDataset, PyTorchDataDataset\n",
    "\n",
    "# Tell pfl to use pytorch as the backend ML framework\n",
    "set_framework_module(pytorch_ops, old_module=numpy_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch from `PyTorchTensorDataset`: \n",
      "x shape=torch.Size([5, 4]), x=tensor([[-1.2608, -1.1227, -1.7867, -1.6763],\n",
      "        [ 0.3639,  0.0880,  0.4456,  0.3723],\n",
      "        [ 1.6437, -0.6749,  1.6947,  1.1746],\n",
      "        [ 0.0336,  0.7171,  0.2515,  0.3695],\n",
      "        [-1.2031, -0.8460, -1.6381, -1.4931]], dtype=torch.float64)\n",
      "y shape=torch.Size([5]), y=tensor([0, 1, 1, 1, 0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Create a PyTorch pfl Dataset object based on PyTorch tensors\n",
    "user_features, user_labels = torch.as_tensor(user_features), torch.as_tensor(user_labels)\n",
    "user_dataset = PyTorchTensorDataset(tensors=[user_features, user_labels], user_id=user_id)\n",
    "\n",
    "# Get a batch of the user dataset using Dataset.iter\n",
    "batch_x, batch_y = next(user_dataset.iter(batch_size=5))\n",
    "print(f\"Batch from `PyTorchTensorDataset`: \\n\"\n",
    "      f\"x shape={batch_x.shape}, x={batch_x}\\n\"\n",
    "      f\"y shape={batch_y.shape}, y={batch_y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch from `PyTorchDataDataset`: \n",
      "x shape=torch.Size([5, 4]), x=tensor([[-1.2608, -1.1227, -1.7867, -1.6763],\n",
      "        [ 0.3639,  0.0880,  0.4456,  0.3723],\n",
      "        [ 1.6437, -0.6749,  1.6947,  1.1746],\n",
      "        [ 0.0336,  0.7171,  0.2515,  0.3695],\n",
      "        [-1.2031, -0.8460, -1.6381, -1.4931]], dtype=torch.float64)\n",
      "y shape=torch.Size([5]), y=tensor([0, 1, 1, 1, 0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Option 2: Create a PyTorch pfl Dataset object based on torch.utils.data.Dataset\n",
    "pytorch_data = torch.utils.data.TensorDataset(user_features, user_labels)\n",
    "user_dataset = PyTorchDataDataset(raw_data=pytorch_data, user_id=user_id)\n",
    "\n",
    "# Get a batch of the user dataset using Dataset.iter\n",
    "batch_x, batch_y = next(user_dataset.iter(batch_size=5))\n",
    "print(f\"\\nBatch from `PyTorchDataDataset`: \\n\"\n",
    "      f\"x shape={batch_x.shape}, x={batch_x}\\n\"\n",
    "      f\"y shape={batch_y.shape}, y={batch_y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating Federated Dataset with Real User Partition\n",
    "Previous section showed how to create a single user dataset.  This section covers how to create a federated dataset which is a joint of multiple users' dataset. If the original dataset already have an attribute for user identifier, such as [StackOverflow](https://www.kaggle.com/datasets/stackoverflow/stackoverflow), [LEAF](https://leaf.cmu.edu) ans [FLAIR](https://github.com/apple/ml-flair), we can use the existing user parition in the dataset for more accurate simulation of non-IID characteristics in the real federated learning setting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FederatedDataset with real user partition: \n",
      "\tReal user with user_id=11 has size of 12.\n",
      "\tReal user with user_id=12 has size of 13.\n",
      "\tReal user with user_id=15 has size of 16.\n",
      "\tReal user with user_id=17 has size of 18.\n",
      "\tReal user with user_id=12 has size of 13.\n"
     ]
    }
   ],
   "source": [
    "from pfl.data import get_user_sampler, FederatedDataset\n",
    "\n",
    "# Create a dataset partitioned by user\n",
    "user_id_to_data = {}\n",
    "n_users = 20\n",
    "for i in range(n_users):\n",
    "    user_id_to_data[i] = sklearn.datasets.make_classification(n_samples=i+1, n_features=4)\n",
    "user_ids = list(user_id_to_data.keys())\n",
    "\n",
    "# Create user sampler to sample user uniformly at random\n",
    "user_sampler = get_user_sampler(sample_type=\"random\", user_ids=user_ids)\n",
    "federated_dataset = FederatedDataset.from_slices(data=user_id_to_data, user_sampler=user_sampler)\n",
    "\n",
    "# Iterate federated dataset to get artificial user dataset\n",
    "print(\"FederatedDataset with real user partition: \")\n",
    "for i in range(5):\n",
    "    user_dataset, seed = next(federated_dataset)\n",
    "    print(f\"\\tReal user with user_id={user_dataset.user_id} has size of {len(user_dataset)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Creating a `PyTorchFederatedDataset` Object\n",
    "In certain scenario, it might be beneficial to use `PyTorchFederatedDataset` for speeding up data loading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorchFederatedDataset with real user partition: \n",
      "\tReal user 0 has size of 20.\n",
      "\tReal user 1 has size of 11.\n",
      "\tReal user 2 has size of 15.\n",
      "\tReal user 3 has size of 14.\n",
      "\tReal user 4 has size of 2.\n"
     ]
    }
   ],
   "source": [
    "from pfl.data.pytorch import PyTorchFederatedDataset\n",
    "\n",
    "class PyTorchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, user_id_to_data):\n",
    "        self._user_id_to_data = user_id_to_data\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return [torch.as_tensor(x) for x in self._user_id_to_data[i]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._user_id_to_data)\n",
    "\n",
    "\n",
    "pytorch_dataset = PyTorchDataset(user_id_to_data)\n",
    "federated_dataset = PyTorchFederatedDataset(dataset=pytorch_dataset, user_sampler=user_sampler)\n",
    "# Iterate federated dataset to get artificial user dataset\n",
    "print(\"PyTorchFederatedDataset with real user partition: \")\n",
    "for i in range(5):\n",
    "    user_dataset, seed = next(federated_dataset)\n",
    "    print(f\"\\tReal user {i} has size of {len(user_dataset)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Creating a `FederatedDataset` with `torch.utils.data.Dataset`\n",
    "If a user dataset is too large to fit into memory, `pfl` also supports `torch.utils.data.Dataset` for creating federated dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FederatedDataset with torch.utils.data.Dataset and real user partition: \n",
      "\tReal user 0 has size of 3.\n",
      "\tReal user 1 has size of 20.\n",
      "\tReal user 2 has size of 9.\n",
      "\tReal user 3 has size of 3.\n",
      "\tReal user 4 has size of 12.\n"
     ]
    }
   ],
   "source": [
    "def make_dataset_fn(user_id):\n",
    "    # Create a `torch.utils.data.Dataset` for a single user\n",
    "    user_data = [torch.as_tensor(data) for data in user_id_to_data[user_id]]\n",
    "    pytorch_data = torch.utils.data.TensorDataset(*user_data)\n",
    "    return PyTorchDataDataset(raw_data=pytorch_data, user_id=user_id)\n",
    "\n",
    "federated_dataset = FederatedDataset(make_dataset_fn=make_dataset_fn, user_sampler=user_sampler)\n",
    "print(\"FederatedDataset with torch.utils.data.Dataset and real user partition: \")\n",
    "for i in range(5):\n",
    "    user_dataset, seed = next(federated_dataset)\n",
    "    print(f\"\\tReal user {i} has size of {len(user_dataset)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creating IID Artificial Federated Dataset\n",
    "The previous section assumes that a dataset has user IDs, which is often not the case for many existing ML datasets (e.g. CIFAR10). For those datasets, `pfl` supports converting them to Artificial Federated Dataset, meaning that there is no user ID associated with the dataset and the user partition will be artificial, which is useful for experimenting existing ML datasets. This section covers how to create an Artificial Federated Dataset assuming the data distribution between users is IID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User dataset size is a constant: \n",
      "\tArtificial user 0 has size of 10\n",
      "\tArtificial user 1 has size of 10\n",
      "\tArtificial user 2 has size of 10\n",
      "\tArtificial user 3 has size of 10\n",
      "\tArtificial user 4 has size of 10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pfl.data import ArtificialFederatedDataset, get_data_sampler\n",
    "\n",
    "\n",
    "n_samples = 1000\n",
    "features, labels = sklearn.datasets.make_classification(n_samples=n_samples, n_features=8, n_informative=4, n_classes=5)\n",
    "# Create data sampler to sample each artificial user dataset as a random subset of the original dataset\n",
    "data_sampler = get_data_sampler(sample_type=\"random\", max_bound=n_samples)\n",
    "\n",
    "# Option 1: Create an artificial federated dataset where each user dataset has constant size of 10\n",
    "sample_dataset_len = lambda: 10\n",
    "federated_dataset = ArtificialFederatedDataset.from_slices(\n",
    "    data=[features, labels], \n",
    "    data_sampler=data_sampler,\n",
    "    sample_dataset_len=sample_dataset_len,\n",
    ")\n",
    "# Iterate federated dataset to get artificial user dataset\n",
    "print(\"User dataset size is a constant: \")\n",
    "for i in range(5):\n",
    "    user_dataset, seed = next(federated_dataset)\n",
    "    print(f\"\\tArtificial user {i} has size of {len(user_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User dataset size follows a Poisson distribution: \n",
      "\tArtificial user 0 has size of 9\n",
      "\tArtificial user 1 has size of 8\n",
      "\tArtificial user 2 has size of 7\n",
      "\tArtificial user 3 has size of 11\n",
      "\tArtificial user 4 has size of 17\n"
     ]
    }
   ],
   "source": [
    "# Option 2: Create an artificial federated dataset where each user dataset size follows Poisson distribution\n",
    "sample_dataset_len = lambda: max(1, np.random.poisson(10))\n",
    "federated_dataset = ArtificialFederatedDataset.from_slices(\n",
    "    data=[features, labels], \n",
    "    data_sampler=data_sampler,\n",
    "    sample_dataset_len=sample_dataset_len,\n",
    ")\n",
    "# Iterate federated dataset to get artificial user dataset\n",
    "print(\"\\nUser dataset size follows a Poisson distribution: \")\n",
    "for i in range(5):\n",
    "    user_dataset, seed = next(federated_dataset)\n",
    "    print(f\"\\tArtificial user {i} has size of {len(user_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Creating non-IID Artificial Federated Dataset\n",
    "As mentioned before, in the real federated setting, the federated data has many non-IID characteristics in its distribution. This section shows how to artificially simulate the case for a classification dataset where the user label distribution is non-IID and follows a Dirichlet distribution (a common assumption in federated learning research). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Sampling non-IID user dataset dynamically with `ArtificialFederatedDataset`\n",
    "The first option is to change the `data_sampler` in a way so that the user label distribution follows a Dirichlet distribution. The user dataset is sampled on the fly and there is no fixed user partition like as in [Section 3.2](#3.2-Partitioning-the-dataset-into-fixed-artificial-users-with-FederatedDataset) below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArtificialFederatedDataset with Dirichlet label distribution: \n",
      "\tArtificial user 0 has size of 20 with unique labels=[1 4]\n",
      "\tArtificial user 1 has size of 20 with unique labels=[3]\n",
      "\tArtificial user 2 has size of 20 with unique labels=[0]\n",
      "\tArtificial user 3 has size of 20 with unique labels=[2 4]\n",
      "\tArtificial user 4 has size of 20 with unique labels=[0]\n"
     ]
    }
   ],
   "source": [
    "# Create data sampler to sample each artificial user has label distribution follows a Dirichlet distribution.\n",
    "dirichlet_alpha = [0.1] * 5\n",
    "data_sampler = get_data_sampler(sample_type=\"dirichlet\", labels=labels, alpha=dirichlet_alpha)\n",
    "\n",
    "# Create an artificial federated dataset with a Dirichlet data sampler\n",
    "sample_dataset_len = lambda: 20\n",
    "federated_dataset = ArtificialFederatedDataset.from_slices(\n",
    "    data=[features, labels], \n",
    "    data_sampler=data_sampler,\n",
    "    sample_dataset_len=sample_dataset_len,\n",
    ")\n",
    "# Iterate federated dataset to get artificial user dataset\n",
    "print(\"ArtificialFederatedDataset with Dirichlet label distribution: \")\n",
    "for i in range(5):\n",
    "    user_dataset, seed = next(federated_dataset)\n",
    "    print(f\"\\tArtificial user {i} has size of {len(user_dataset)} with unique labels={np.unique(user_dataset.raw_data[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Partitioning the dataset into fixed artificial users with `FederatedDataset`\n",
    "An alternative to the above is to partition the original dataset into a fixed set of artificial users where each user's label distribution follows a Dirichlet distribution. The difference of this option is that the user partitions are constructed once using all data and remains the same throughout the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FederatedDataset with Dirichlet label distribution: \n",
      "\tArtificial user 0 has size of 20 with unique labels=[0 3]\n",
      "\tArtificial user 1 has size of 20 with unique labels=[1]\n",
      "\tArtificial user 2 has size of 20 with unique labels=[3]\n",
      "\tArtificial user 3 has size of 20 with unique labels=[2]\n",
      "\tArtificial user 4 has size of 20 with unique labels=[2 4]\n"
     ]
    }
   ],
   "source": [
    "# Create a federated dataset with fixed user partition\n",
    "# alpha=0.01 is a extreme case where each user likely has only data from 1 class\n",
    "federated_dataset = FederatedDataset.from_slices_with_dirichlet_class_distribution(\n",
    "    data=[features, labels],\n",
    "    labels=labels,\n",
    "    alpha=0.01,\n",
    "    user_dataset_len_sampler=sample_dataset_len)\n",
    "\n",
    "# Iterate federated dataset to get artificial user dataset\n",
    "print(\"FederatedDataset with Dirichlet label distribution: \")\n",
    "for i in range(5):\n",
    "    user_dataset, seed = next(federated_dataset)\n",
    "    print(f\"\\tArtificial user {i} has size of {len(user_dataset)} with unique labels={np.unique(user_dataset.raw_data[1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coupling CIFAR10 data to Federated Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging print statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21.0+cpu\n",
      "c:\\Users\\AVN\\anaconda3\\envs\\masters\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "print(torchvision.__version__)\n",
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from centralized import load_data\n",
    "train_set, test_set = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the 'dataset' attribute of train_set to get the proper count of total samples\n",
    "len(train_set.dataset)\n",
    "\n",
    "# Just 'len(train_set)' would only give you the total number of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set.dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
