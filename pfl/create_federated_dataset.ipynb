{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creating a `pfl` Dataset Object\n",
    "A `pfl.data.Dataset` object is the basic representation for a single user's dataset. Alternatively, this can also be used for building a central dataset for evaluation purpose (to be used with `pfl.callback.CentralEvaluationCallback`).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.26082601 -1.12266244 -1.78670903 -1.67631867]\n",
      " [ 0.36388835  0.08802618  0.44563335  0.37226589]\n",
      " [ 1.64369741 -0.67493593  1.69465502  1.17460049]\n",
      " [ 0.03360219  0.71709835  0.25154238  0.36946786]\n",
      " [-1.20314452 -0.84598324 -1.6381041  -1.49313281]\n",
      " [ 1.4854961  -1.81824661  1.17298631  0.49046328]\n",
      " [-0.23414879  0.30232747 -0.18022215 -0.06987397]\n",
      " [-2.68763717  2.32659373 -2.40802671 -1.34256486]\n",
      " [-0.38952767  0.76608295 -0.22172902  0.00812763]\n",
      " [-1.05540026 -1.35143849 -1.61777354 -1.59778114]]\n",
      "Batch from `Dataset`: \n",
      "x shape=(5, 4), x=[[-1.26082601 -1.12266244 -1.78670903 -1.67631867]\n",
      " [ 0.36388835  0.08802618  0.44563335  0.37226589]\n",
      " [ 1.64369741 -0.67493593  1.69465502  1.17460049]\n",
      " [ 0.03360219  0.71709835  0.25154238  0.36946786]\n",
      " [-1.20314452 -0.84598324 -1.6381041  -1.49313281]]\n",
      "y shape=(5,), y=[0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets\n",
    "from pfl.data.dataset import Dataset\n",
    "\n",
    "# Create synthetic user data\n",
    "user_id = 'pfl-user-0'\n",
    "user_features, user_labels = sklearn.datasets.make_classification(n_samples=10, n_features=4)\n",
    "\n",
    "# Create a pfl Dataset object\n",
    "user_dataset = Dataset(raw_data=[user_features, user_labels], user_id=user_id)\n",
    "\n",
    "# Get a batch of the user dataset using Dataset.iter\n",
    "batch_x, batch_y = next(user_dataset.iter(batch_size=5))\n",
    "print(f\"Batch from `Dataset`: \\n\"\n",
    "      f\"x shape={batch_x.shape}, x={batch_x}\\n\"\n",
    "      f\"y shape={batch_y.shape}, y={batch_y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Creating a PyTorch `pfl` Dataset Object\n",
    "The above example showed how to create the `pfl` dataset in numpy format. `pfl`  supports data that are processed in PyTorch or TensorFlow tensors. We will focus on PyTorch for the purpose of this tutorial. The TensorFlow dataset implementation can be found at [tensorflow.py](https://github.com/apple/pfl-research/blob/main/pfl/data/tensorflow.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foo\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "os.environ['PFL_PYTORCH_DEVICE'] = 'cpu'\n",
    "\n",
    "import torch\n",
    "\n",
    "# Set multiprocessing start method to \"spawn\" instead of forkserver (which is the default)\n",
    "# That is because forkserver does not work on Windows, but spawn does.\n",
    "def init_multiprocessing():\n",
    "    try:\n",
    "        multiprocessing.set_start_method(\"spawn\", force=True)  # Forces \"spawn\"\n",
    "    except RuntimeError:\n",
    "        pass  # Ignore if it's already set\n",
    "\n",
    "init_multiprocessing()\n",
    "\n",
    "from pfl.internal.ops.selector import set_framework_module\n",
    "from pfl.internal.ops import pytorch_ops, numpy_ops\n",
    "from pfl.data.pytorch import PyTorchTensorDataset, PyTorchDataDataset\n",
    "\n",
    "# Tell pfl to use pytorch as the backend ML framework\n",
    "set_framework_module(pytorch_ops, old_module=numpy_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch from `PyTorchTensorDataset`: \n",
      "x shape=torch.Size([5, 4]), x=tensor([[-1.2608, -1.1227, -1.7867, -1.6763],\n",
      "        [ 0.3639,  0.0880,  0.4456,  0.3723],\n",
      "        [ 1.6437, -0.6749,  1.6947,  1.1746],\n",
      "        [ 0.0336,  0.7171,  0.2515,  0.3695],\n",
      "        [-1.2031, -0.8460, -1.6381, -1.4931]], dtype=torch.float64)\n",
      "y shape=torch.Size([5]), y=tensor([0, 1, 1, 1, 0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Create a PyTorch pfl Dataset object based on PyTorch tensors\n",
    "user_features, user_labels = torch.as_tensor(user_features), torch.as_tensor(user_labels)\n",
    "user_dataset = PyTorchTensorDataset(tensors=[user_features, user_labels], user_id=user_id)\n",
    "\n",
    "# Get a batch of the user dataset using Dataset.iter\n",
    "batch_x, batch_y = next(user_dataset.iter(batch_size=5))\n",
    "print(f\"Batch from `PyTorchTensorDataset`: \\n\"\n",
    "      f\"x shape={batch_x.shape}, x={batch_x}\\n\"\n",
    "      f\"y shape={batch_y.shape}, y={batch_y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch from `PyTorchDataDataset`: \n",
      "x shape=torch.Size([5, 4]), x=tensor([[-1.2608, -1.1227, -1.7867, -1.6763],\n",
      "        [ 0.3639,  0.0880,  0.4456,  0.3723],\n",
      "        [ 1.6437, -0.6749,  1.6947,  1.1746],\n",
      "        [ 0.0336,  0.7171,  0.2515,  0.3695],\n",
      "        [-1.2031, -0.8460, -1.6381, -1.4931]], dtype=torch.float64)\n",
      "y shape=torch.Size([5]), y=tensor([0, 1, 1, 1, 0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Option 2: Create a PyTorch pfl Dataset object based on torch.utils.data.Dataset\n",
    "pytorch_data = torch.utils.data.TensorDataset(user_features, user_labels)\n",
    "user_dataset = PyTorchDataDataset(raw_data=pytorch_data, user_id=user_id)\n",
    "\n",
    "# Get a batch of the user dataset using Dataset.iter\n",
    "batch_x, batch_y = next(user_dataset.iter(batch_size=5))\n",
    "print(f\"\\nBatch from `PyTorchDataDataset`: \\n\"\n",
    "      f\"x shape={batch_x.shape}, x={batch_x}\\n\"\n",
    "      f\"y shape={batch_y.shape}, y={batch_y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating Federated Dataset with Real User Partition\n",
    "Previous section showed how to create a single user dataset.  This section covers how to create a federated dataset which is a joint of multiple users' dataset. If the original dataset already have an attribute for user identifier, such as [StackOverflow](https://www.kaggle.com/datasets/stackoverflow/stackoverflow), [LEAF](https://leaf.cmu.edu) ans [FLAIR](https://github.com/apple/ml-flair), we can use the existing user parition in the dataset for more accurate simulation of non-IID characteristics in the real federated learning setting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FederatedDataset with real user partition: \n",
      "\tReal user with user_id=11 has size of 12.\n",
      "\tReal user with user_id=12 has size of 13.\n",
      "\tReal user with user_id=15 has size of 16.\n",
      "\tReal user with user_id=17 has size of 18.\n",
      "\tReal user with user_id=12 has size of 13.\n"
     ]
    }
   ],
   "source": [
    "from pfl.data import get_user_sampler, FederatedDataset\n",
    "\n",
    "# Create a dataset partitioned by user\n",
    "user_id_to_data = {}\n",
    "n_users = 20\n",
    "for i in range(n_users):\n",
    "    user_id_to_data[i] = sklearn.datasets.make_classification(n_samples=i+1, n_features=4)\n",
    "user_ids = list(user_id_to_data.keys())\n",
    "\n",
    "# Create user sampler to sample user uniformly at random\n",
    "user_sampler = get_user_sampler(sample_type=\"random\", user_ids=user_ids)\n",
    "federated_dataset = FederatedDataset.from_slices(data=user_id_to_data, user_sampler=user_sampler)\n",
    "\n",
    "# Iterate federated dataset to get artificial user dataset\n",
    "print(\"FederatedDataset with real user partition: \")\n",
    "for i in range(5):\n",
    "    user_dataset, seed = next(federated_dataset)\n",
    "    print(f\"\\tReal user with user_id={user_dataset.user_id} has size of {len(user_dataset)}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
