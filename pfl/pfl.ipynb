{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Both Jupyter and `pfl` use async. `nest_asyncio` allows `pfl` to run inside the notebook \n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# append the root directory to your paths to be able to reach the examples.  \n",
    "torch.random.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# Always import the `pfl` model first before initializing any `pfl` components to let `pfl` know which Deep Learning framework you will use.\n",
    "import multiprocessing\n",
    "# Set multiprocessing start method to \"spawn\" instead of forkserver (which is the default)\n",
    "# That is because forkserver does not work on Windows, but spawn does.\n",
    "def init_multiprocessing():\n",
    "    try:\n",
    "        multiprocessing.set_start_method(\"spawn\", force=True)  # Forces \"spawn\"\n",
    "    except RuntimeError:\n",
    "        pass  # Ignore if it's already set\n",
    "\n",
    "init_multiprocessing()\n",
    "\n",
    "from pfl.model.pytorch import PyTorchModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Use the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters DPFL\n",
    "\n",
    "Adjust these values as you please"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfl.privacy import RDPPrivacyAccountant, CentrallyAppliedPrivacyMechanism, GaussianMechanism\n",
    "\n",
    "\n",
    "# # Define Gaussian DP mechanisms for central DP guarantees using three different methods\n",
    "\n",
    "# n_clients = 8                # Number of clients\n",
    "# clipping_bound = 0.5         # Clipping bound for DP\n",
    "# epsilon = 2                  # Epsilon parameter for DP\n",
    "# delta = 1e-8                 # Delta parameter for DP\n",
    "# sampling_probability = 1e-4  # Probability of a client being selected as cohort for a given iteration: cohort_size / n_clients\n",
    "# is_central = True            # Probably delete this line in the future, as it is not used anywhere else in the code.\n",
    "\n",
    "# # Other parameters for the training process\n",
    "\n",
    "# local_learning_rate = 0.001  # Learning rate for local training\n",
    "# local_num_epochs = 4         # Number of local epochs for each client\n",
    "# local_batch_size = 32        # Batch size for local training\n",
    "# central_num_iterations = 50  # Number of central iterations\n",
    "# evaluation_frequency = 1     # How often to evaluate the model (in terms of central iterations)\n",
    "# cohort_size = 8              # Number of clients in the cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the DP mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.gautamkamath.com/CS860notes/lec5.pdf\n",
    "\n",
    "Definition 5 on page 3 gives the definition of the parameters for Gaussian distribution where we sample noise for DP.\n",
    "\n",
    "$\\Delta_2^2$ is not included when defining `relative_noise_stddev`, because the clipping bound (which I understand to be $\\Delta_2^2$ and also is $\\Delta_2^{(f)}$ from definition 3 on page 3) is multiplied on `relative_noise_stddev`, when sampling the noise for DP via `GaussianMechanism.add_noise()`\n",
    "\n",
    "Source: https://apple.github.io/pfl-research/reference/privacy.html#pfl.privacy.gaussian_mechanism.GaussianMechanism\n",
    "\n",
    "Additionally, the issue here is how we select a good value of clipping bound. Normally, you set it to be the sensitivity $\\Delta_2^{(f)}$ of function $f$ that aggregates on the dataset. But in our context of FL training, the function in question is the one that computes the gradients. Main problem with this is that the output gradient can be arbitrarily big, so the sensitiviy is in theory infinite. <br>\n",
    "We need to figure out ourselves how to pick a good value for clipping bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# relative_noise_stddev = 2 * math.log(1.25 / delta) * 1/(epsilon**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLD stands for privacy loss distribution.\n",
    "A privacy accountant serves the purpose of reducing privacy leakage that is incurred from accessing the same data over the span of several central FL iterations.\n",
    "As a result, it should give stronger privacy. It is based on the composability of Differential Privacy.\n",
    "\n",
    "If you wish to use a privacy accountant attached to your DP mechanism, then you may uncomment the following code cell and then add `pld_central_privacy` as an element in `postprocessors` variable, which is found in the section 'Setting up the model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A privacy accountant needs the following parameters when initialized:\n",
    "- `num_compositions: int`: that is the total number of central iterations in FL training.\n",
    "- `sampling_probability: float`: the probability that a user is chosen to be part of cohorts in a given iteration, i.e. `cohort_size / population`\n",
    "- `mechanism: str`: specifies which noise mechanism to use. `'gaussian'` or `'laplace'` (according to source code, these are the only valid arguments).\n",
    "- `epsilon: Optional[float]`: your desired DP parameter of your choice.\n",
    "- `delta: Optional[float]`: your desired DP parameter of your choice.\n",
    "- `noise_scale: float = 1.0`: I think this is multiplied on the sampled noise and the result is added to the gradient. By default, it is set to 1.0 and I think it would be best to not meddle with it.\n",
    "\n",
    "Regarding to `epsilon` and `delta`, the privacy accountant will strive to apply the chosen `mechanism` during training while the privacy loss stays within the two privacy parameters. So `epsilon` and `delta` are our privacy budget that we choose freely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacy Loss Distribution Accountant\n",
    "\n",
    "This one was from the `pfl-research` tutorial: https://colab.research.google.com/github/apple/pfl-research/blob/develop/tutorials/Introduction%20to%20Differential%20Privacy%20with%20Federated%20Learning.ipynb#scrollTo=o9o_TZL3JUp9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define a Gaussian DP mechanism using the PLD privacy accountant\n",
    "# # WARNING: it takes a while for the gaussian_moments_accountant mechanism to be instantiated\n",
    "\n",
    "# from pfl.privacy import (PLDPrivacyAccountant, CentrallyAppliedPrivacyMechanism, GaussianMechanism, LocalPrivacyMechanism)\n",
    "\n",
    "# # define the PLD privacy accountant, which will use the Gaussian noise mechanism\n",
    "# pld_accountant = PLDPrivacyAccountant(\n",
    "#     num_compositions=num_epochs,\n",
    "#     sampling_probability=sampling_probability,\n",
    "#     mechanism='gaussian',\n",
    "#     epsilon=epsilon,\n",
    "#     delta=delta)\n",
    "\n",
    "# # instantiate a Gaussian noise mechanism using the privacy accountant\n",
    "# pld_gaussian_noise_mechanism = GaussianMechanism.from_privacy_accountant(\n",
    "#     accountant=pld_accountant, clipping_bound=clipping_bound)\n",
    "\n",
    "# # wrap the noise mechanism with CentrallyAppliedPrivacyMechanism to make it a central privacy mechanism\n",
    "# pld_central_privacy = CentrallyAppliedPrivacyMechanism(pld_gaussian_noise_mechanism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renyi Differential Privacy Accountant\n",
    "\n",
    "Hannah recommends this accountant as a starter on involving DP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdp_accountant = RDPPrivacyAccountant(\n",
    "#     num_compositions=central_num_iterations,\n",
    "#     sampling_probability=sampling_probability,\n",
    "#     mechanism='gaussian',\n",
    "#     epsilon=epsilon,\n",
    "#     delta=delta\n",
    "# )\n",
    "\n",
    "# rdp_gaussian_noise_mechanism = GaussianMechanism.from_privacy_accountant(\n",
    "#     accountant=rdp_accountant, clipping_bound=clipping_bound)\n",
    "\n",
    "# # You then add this to the 'postprocessors' array instead of pld_central_privacy.\n",
    "# rdp_central_privacy = CentrallyAppliedPrivacyMechanism(rdp_gaussian_noise_mechanism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Local DP mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://apple.github.io/pfl-research/reference/privacy.html#pfl.privacy.gaussian_mechanism.GaussianMechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an attempt to define our own custom DP mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pfl.metrics import Metrics\n",
    "\n",
    "# class LocallyAppliedPrivacyMechanism(LocalPrivacyMechanism):\n",
    "\n",
    "#     def privatize(self, statistics, name_formatting_fn=..., seed = None):\n",
    "#         # TODO: Implement actual privatization for local DP\n",
    "#         # TODO: Sample some noise (Gaussian: parameters depend on privacy parameters)\n",
    "#         gaussian_mechanism = GaussianMechanism(clipping_bound=clipping_bound, \n",
    "#                                                relative_noise_stddev=relative_noise_stddev)\n",
    "#         noisy_statistics, metrics = gaussian_mechanism.add_noise(statistics=statistics, \n",
    "#                                      cohort_size=cohort_size, \n",
    "#                                      name_formatting_fn=name_formatting_fn)\n",
    "#         # TODO: Add the noise to statistics then send it\n",
    "#         return noisy_statistics, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can try to use the implementation of privacy mechanisms that already exist in `pfl` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfl.privacy import GaussianMechanism, LaplaceMechanism\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model for CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, Optional\n",
    "from pfl.metrics import Weighted\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 256)  # Adjusted for CIFAR-10\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "        \n",
    "        self.batch_norm1 = nn.BatchNorm2d(32)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(64)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.batch_norm1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.batch_norm2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.batch_norm3(self.conv3(x))))\n",
    "        \n",
    "        x = torch.flatten(x, 1)  # Flatten feature maps\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc3(x)\n",
    "    \n",
    "\n",
    "    def loss(self, inputs: torch.Tensor, targets: torch.Tensor, eval: bool = False) -> torch.Tensor:\n",
    "        self.eval() if eval else self.train()\n",
    "        return loss_fn(self(inputs), targets)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def metrics(self,\n",
    "                inputs: torch.Tensor,\n",
    "                targets: torch.Tensor,\n",
    "                eval: bool = True) -> Dict[str, Weighted]:\n",
    "        self.eval() if eval else self.train()\n",
    "        prediction = self(inputs)\n",
    "        logits = torch.argmax(prediction, dim=1)\n",
    "        num_samples = len(inputs)\n",
    "        # print(f\"num_samples: {num_samples}\")\n",
    "        # print(f'targets: {targets}')\n",
    "        # print(f'targets shape: {targets.shape}')\n",
    "        # print(f'logits: {logits}')\n",
    "        # print(f'logits shape: {logits.shape}')\n",
    "        # print(f'prediction: {prediction}')\n",
    "        # print(f'prediction shape: {prediction.shape}')\n",
    "        # row_indices = torch.arange(targets.size(0))\n",
    "        # correct_vector = targets[row_indices,logits]\n",
    "        # print(f'correct_vector: {correct_vector}')\n",
    "        # print(f'correct_vector shape: {correct_vector.shape}')\n",
    "\n",
    "        # temp = torch.eq(logits, targets)\n",
    "        # correct = torch.sum(correct_vector)\n",
    "\n",
    "        # loss = loss_fn(prediction, targets).item()\n",
    "\n",
    "        # Co-pilot fix\n",
    "        if targets.ndim > 1:\n",
    "            targets_indices = torch.argmax(targets, dim=1)\n",
    "        else:\n",
    "            targets_indices = targets\n",
    "        correct = (logits == targets_indices).sum().item()\n",
    "        loss = loss_fn(prediction, targets_indices).item()\n",
    "        \n",
    "        return {\n",
    "            \"loss\": Weighted(loss, num_samples),\n",
    "            \"accuracy\": Weighted(correct, num_samples)\n",
    "        }\n",
    "\n",
    "\n",
    "pytorch_model = Net()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#pytorch_model.loss = loss\n",
    "#pytorch_model.metrics = metrics\n",
    "\n",
    "#pytorch_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative model from Ruben\n",
    "class Net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64,  kernel_size=5, padding=2)   # 4 800 + 64 = 4 864\n",
    "        self.conv2 = nn.Conv2d(64,128, kernel_size=5, padding=2)   # 204 800 + 128 = 204 928\n",
    "        self.pool  = nn.MaxPool2d(2)                               # keeps code tidy\n",
    "        self.fc1   = nn.Linear(128*6*6, 192)                       # 884 736 + 192\n",
    "        self.fc2   = nn.Linear(192,  64)                           # 12 288 + 64\n",
    "        self.out   = nn.Linear(64,   10)                           # 640 + 10\n",
    "        # ≈1 107 700 trainable parameters in total\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))   # 24→12\n",
    "        x = self.pool(F.relu(self.conv2(x)))   # 12→6\n",
    "        x = torch.flatten(x, 1)                # 128 × 6 × 6 = 4 608 features\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.out(x)\n",
    "    \n",
    "    def loss(self, inputs: torch.Tensor, targets: torch.Tensor, eval: bool = False) -> torch.Tensor:\n",
    "        self.eval() if eval else self.train()\n",
    "        return loss_fn(self(inputs), targets)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def metrics(self,\n",
    "                inputs: torch.Tensor,\n",
    "                targets: torch.Tensor,\n",
    "                eval: bool = True) -> Dict[str, Weighted]:\n",
    "        self.eval() if eval else self.train()\n",
    "        prediction = self(inputs)\n",
    "        logits = torch.argmax(prediction, dim=1)\n",
    "        num_samples = len(inputs)\n",
    "        if targets.ndim > 1:\n",
    "            targets_indices = torch.argmax(targets, dim=1)\n",
    "        else:\n",
    "            targets_indices = targets\n",
    "        correct = (logits == targets_indices).sum().item()\n",
    "        loss = loss_fn(prediction, targets_indices).item()\n",
    "        \n",
    "        return {\n",
    "            \"loss\": Weighted(loss, num_samples),\n",
    "            \"accuracy\": Weighted(correct, num_samples)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetMNIST(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = x.size(0)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(batch_size, -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "    def loss(self, inputs: torch.Tensor, targets: torch.Tensor, eval: bool = False) -> torch.Tensor:\n",
    "        self.eval() if eval else self.train()\n",
    "        return loss_fn(self(inputs), targets)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def metrics(self,\n",
    "                inputs: torch.Tensor,\n",
    "                targets: torch.Tensor,\n",
    "                eval: bool = True) -> Dict[str, Weighted]:\n",
    "        self.eval() if eval else self.train()\n",
    "        prediction = self(inputs)\n",
    "        logits = torch.argmax(prediction, dim=1)\n",
    "        num_samples = len(inputs)\n",
    "        if targets.ndim > 1:\n",
    "            targets_indices = torch.argmax(targets, dim=1)\n",
    "        else:\n",
    "            targets_indices = targets\n",
    "        correct = (logits == targets_indices).sum().item()\n",
    "        loss = loss_fn(prediction, targets_indices).item()\n",
    "        \n",
    "        return {\n",
    "            \"loss\": Weighted(loss, num_samples),\n",
    "            \"accuracy\": Weighted(correct, num_samples)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper function for instantiating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pytorch_model():\n",
    "    pytorch_model = NetMNIST().to(device)\n",
    "    pytorch_model.loss = pytorch_model.loss\n",
    "    pytorch_model.metrics = pytorch_model.metrics\n",
    "    return pytorch_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfl.data.dataset import Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ./data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5,), std=(0.5,))\n",
       "           )"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: ./data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5,), std=(0.5,))\n",
       "           )"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader_to_tensor_pair(dataloader):\n",
    "    \"\"\"\n",
    "    Converts a PyTorch DataLoader to a pair of tensors (inputs, targets).\n",
    "    Args:\n",
    "        dataloader (torch.utils.data.DataLoader): The DataLoader to convert.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing two tensors: inputs and targets.\"\"\"\n",
    "    \n",
    "    # print(\"dataloader_to_tensor_pair called\")\n",
    "    inputs, targets = [], []\n",
    "    # i = 0\n",
    "    # print(\"Starting to iterate over the dataloader\")\n",
    "    print(dataloader)\n",
    "    for feature, label in dataloader:\n",
    "        # print(f'Iteration {i}')\n",
    "        # i += 1\n",
    "        inputs.append(feature)\n",
    "        targets.append(label)\n",
    "    return torch.cat(inputs, dim=0), torch.cat(targets, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling dataloader_to_tensor_pair() to convert the train loader to tensors...\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x00000202B99D1DE0>\n",
      "Calling dataloader_to_tensor_pair() to convert the test loader to tensors...\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x00000202B99D3010>\n",
      "Done converting dataloaders to tensors.\n"
     ]
    }
   ],
   "source": [
    "print(\"Calling dataloader_to_tensor_pair() to convert the train loader to tensors...\")\n",
    "train_features, train_labels = dataloader_to_tensor_pair(trainloader)\n",
    "print(\"Calling dataloader_to_tensor_pair() to convert the test loader to tensors...\")\n",
    "val_features, val_labels = dataloader_to_tensor_pair(testloader)\n",
    "print(\"Done converting dataloaders to tensors.\")\n",
    "central_data = Dataset(raw_data=[val_features, val_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways of creating the federated dataset. You create an artificial federated dataset or just a normal federated dataset.\n",
    "\n",
    "The difference between the two is that you want to use artificial federated dataset if your dataset does not provide user identifiers. These user identifiers are used to create an association between a user (participating in FL training) and their local dataset.\n",
    "\n",
    "An example of a dataset that lacks user identifier is the CIFAR10 dataset. Therefore, it would be natural to instantiate an `ArtificialFederatedDataset`. One issue with this class from the `pfl` module is that you do not have any control over how many users there are in the dataset. The way `ArtificialFederatedDataset` works is that whenever you request a user dataset from it, the federated dataset samples a random subset of the original dataset and returns this subset on demand. This subset is an artificial user dataset <br>\n",
    "On the other hand, you can decide how many data examples an artificial user by providing a callable parameter `sample_dataset_len`. It could be a lambda function that returns a constant, like 10 for instance. This means that every artificial user dataset will contain 10 data examples. Or you may do something a bit more complicated and sample a random number from a distribution of your choice. Then not all artificial user datasets will have the same number of examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating artificial federated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfl.data import ArtificialFederatedDataset, get_data_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create data sampler to sample each artificial user dataset as a random subset of the original dataset\n",
    "# data_sampler = get_data_sampler(sample_type=\"minimize_reuse\", max_bound=n_samples)\n",
    "\n",
    "# # Create an artificial federated dataset where each user dataset has constant size such that there are 10 users to distribute among\n",
    "# sample_dataset_len = lambda: int(n_samples/10)\n",
    "# federated_dataset = ArtificialFederatedDataset.from_slices(\n",
    "#     data=[all_features, all_labels], \n",
    "#     data_sampler=data_sampler,\n",
    "#     sample_dataset_len=sample_dataset_len,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_data_sampler = get_data_sampler(sample_type=\"minimize_reuse\", max_bound=val_n_samples)\n",
    "\n",
    "# # Create an artificial federated dataset where each user dataset has constant size such that there are of 10 users to distribute among\n",
    "# val_sample_dataset_len = lambda: int(val_n_samples/10)\n",
    "# val_federated_dataset = ArtificialFederatedDataset.from_slices(\n",
    "#     data=[val_all_features, val_all_labels],\n",
    "#     data_sampler=val_data_sampler,\n",
    "#     sample_dataset_len=val_sample_dataset_len\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a federated dataset\n",
    "\n",
    "For the purpose of being able to control the actual number of users, which `ArtificialFederatedDataset` does not allow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Reflection on designing `FederatedDataset`</strong>\n",
    "\n",
    "How many datapoints should each user have?\n",
    "- Every user has 50000/100 = 500 datapoints (easy approach)\n",
    "- Alternatively, do some randomization on how many datapoints each user has (harder approach)\n",
    "\n",
    "Should we use the exact same federated dataset in every FL experiment?\n",
    "- The datapoint will be allocated to the same user in every experiment. This means that user 0 will have the exact same datapoints. User 1 will also have the exact same datapoints and so on... for every single time we run our experiment. This means our federated dataset will be exactly the same every time we run our experiment.\n",
    "\n",
    "We decided to assume that all users have equally many datapoints and they will each possess the same datapoints every time we run our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfl.data import get_user_sampler, FederatedDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training set (Fixed)\n",
    "I must first decide how many users there are and then the method to how the data is distributed among the users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameter\n",
    "# user_dataset_size = int(n_samples / n_clients)\n",
    "# print(f'datapoints per user: {user_dataset_size}')\n",
    "\n",
    "# # Maps user id to user dataset\n",
    "# user_id_to_data = {}\n",
    "\n",
    "# # Assign each user a dataset\n",
    "# for i in range(n_clients):\n",
    "#     start = i*user_dataset_size\n",
    "#     end = start+user_dataset_size\n",
    "#     features = train_all_features[start:end]\n",
    "#     labels = train_all_labels[start:end]\n",
    "#     user_id_to_data[i] = (features, labels)\n",
    "\n",
    "# user_ids = list(user_id_to_data.keys())\n",
    "\n",
    "# user_sampler = get_user_sampler(sample_type=\"random\", user_ids=user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# federated_dataset = FederatedDataset.from_slices(\n",
    "#     data=user_id_to_data,\n",
    "#     user_sampler=user_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encapsulate the above code in a function\n",
    "\n",
    "def create_fixed_federated_dataset(\n",
    "    n_clients: int,\n",
    "    user_dataset_size: int,\n",
    "    train_all_features: torch.Tensor,\n",
    "    train_all_labels: torch.Tensor\n",
    ") -> FederatedDataset:\n",
    "    \"\"\"\n",
    "    Create a fixed federated dataset with a given number of clients and dataset size.\n",
    "    \n",
    "    Args:\n",
    "        n_clients (int): Number of clients.\n",
    "        user_dataset_size (int): Size of each user's dataset.\n",
    "        train_all_features (torch.Tensor): All training features.\n",
    "        train_all_labels (torch.Tensor): All training labels.\n",
    "        \n",
    "    Returns:\n",
    "        FederatedDataset: The created federated dataset.\n",
    "    \"\"\"\n",
    "    user_id_to_data = {}\n",
    "    \n",
    "    for i in range(n_clients):\n",
    "        start = i * user_dataset_size\n",
    "        end = start + user_dataset_size\n",
    "        features = train_all_features[start:end]\n",
    "        labels = train_all_labels[start:end]\n",
    "        user_id_to_data[i] = (features, labels)\n",
    "\n",
    "    user_ids = list(user_id_to_data.keys())\n",
    "    user_sampler = get_user_sampler(sample_type=\"random\", user_ids=user_ids)\n",
    "\n",
    "    return FederatedDataset.from_slices(\n",
    "        data=user_id_to_data,\n",
    "        user_sampler=user_sampler\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training set (Random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing cells for shuffling torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor = torch.tensor([\n",
    "#     [1, 2, 3],\n",
    "#     [4, 5, 6],\n",
    "#     [7, 8, 9]\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rand_indices = torch.randperm(tensor.size(0))\n",
    "# rand_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffled = tensor[rand_indices]\n",
    "# shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual creation of random `FederatedDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Hyperparameter\n",
    "# user_dataset_size = int(n_samples / n_clients)\n",
    "# print(f'datapoints per user: {user_dataset_size}')\n",
    "\n",
    "# # Maps user id to user dataset\n",
    "# user_id_to_data = {}\n",
    "\n",
    "# # Randomly shuffle the dataset\n",
    "# rand_indices = torch.randperm(n_samples)\n",
    "# shuffled_features = train_all_features[rand_indices]\n",
    "# shuffled_labels = train_all_labels[rand_indices]\n",
    "\n",
    "# # Assign each user a dataset\n",
    "# for i in range(n_clients):\n",
    "#     start = i*user_dataset_size\n",
    "#     end = start+user_dataset_size\n",
    "#     features = shuffled_features[start:end]\n",
    "#     labels = shuffled_labels[start:end]\n",
    "#     user_id_to_data[i] = (features, labels)\n",
    "\n",
    "# user_ids = list(user_id_to_data.keys())\n",
    "\n",
    "# user_sampler = get_user_sampler(sample_type=\"random\", user_ids=user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# federated_dataset = FederatedDataset.from_slices(\n",
    "#     data=user_id_to_data,\n",
    "#     user_sampler=user_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encapsulate the above code in a function\n",
    "\n",
    "def create_random_federated_dataset(\n",
    "    n_clients: int,\n",
    "    user_dataset_size: int,\n",
    "    train_all_features: torch.Tensor,\n",
    "    train_all_labels: torch.Tensor\n",
    ") -> FederatedDataset:\n",
    "    \"\"\"\n",
    "    Create a random federated dataset with a given number of clients and dataset size.\n",
    "    \n",
    "    Args:\n",
    "        n_clients (int): Number of clients.\n",
    "        user_dataset_size (int): Size of each user's dataset.\n",
    "        train_all_features (torch.Tensor): All training features.\n",
    "        train_all_labels (torch.Tensor): All training labels.\n",
    "        \n",
    "    Returns:\n",
    "        FederatedDataset: The created federated dataset.\n",
    "    \"\"\"\n",
    "    user_id_to_data = {}\n",
    "    \n",
    "    # Randomly shuffle the dataset\n",
    "    rand_indices = torch.randperm(train_all_features.size(0))\n",
    "    shuffled_features = train_all_features[rand_indices]\n",
    "    shuffled_labels = train_all_labels[rand_indices]\n",
    "\n",
    "    # Assign each user a dataset\n",
    "    for i in range(n_clients):\n",
    "        start = i * user_dataset_size\n",
    "        end = start + user_dataset_size\n",
    "        features = shuffled_features[start:end]\n",
    "        labels = shuffled_labels[start:end]\n",
    "        user_id_to_data[i] = (features, labels)\n",
    "\n",
    "    user_ids = list(user_id_to_data.keys())\n",
    "    user_sampler = get_user_sampler(sample_type=\"random\", user_ids=user_ids)\n",
    "\n",
    "    return FederatedDataset.from_slices(\n",
    "        data=user_id_to_data,\n",
    "        user_sampler=user_sampler\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Iterable\n",
    "from pfl.data.dataset import AbstractDataset\n",
    "from pfl.data.federated_dataset import FederatedDatasetBase\n",
    "\n",
    "# Define my own custom Federated Dataset class for validation data\n",
    "\n",
    "\"\"\"\n",
    "    I did not see any reason to distribute validation data among users.\n",
    "    Unfortunately, the pfl module does not provide a fitting implementation of\n",
    "    a FederatedDataset for validation data. Therefore, I created my own class\n",
    "    that simply returns the validation data as is.\n",
    "\"\"\"\n",
    "class ValidationFederatedDataset(FederatedDatasetBase):\n",
    "\n",
    "    def __init__(self, dataset: Dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __next__(self) -> Tuple[AbstractDataset, int]:\n",
    "        return Tuple(self.dataset, 0)\n",
    "    \n",
    "    def get_cohort(self, cohort_size: int) -> Iterable[Tuple[AbstractDataset, int]]:\n",
    "        return [(self.dataset, 0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_federated_dataset = ValidationFederatedDataset(Dataset([val_all_features, val_all_labels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import pfl.data.dataset\n",
    "\n",
    "# importlib.reload(pfl.data.dataset)\n",
    "\n",
    "# from pfl.data.dataset import Dataset\n",
    "\n",
    "# cohorts = federated_dataset.get_cohort(1)\n",
    "# print(f'cohorts: {cohorts}')\n",
    "# user_dataset, _ = next(cohorts)\n",
    "# print(f'user_dataset: {user_dataset}')\n",
    "# len(user_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = [p for p in pytorch_model.parameters() if p.requires_grad]\n",
    "\n",
    "# model = PyTorchModel(pytorch_model, \n",
    "#                      local_optimizer_create=torch.optim.Adam,\n",
    "#                      central_optimizer=torch.optim.Adam(params, lr=0.001, weight_decay=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfl.algorithm import FederatedAveraging, NNAlgorithmParams\n",
    "from pfl.callback import CentralEvaluationCallback, AggregateMetricsToDisk\n",
    "from pfl.hyperparam import NNTrainHyperParams, NNEvalHyperParams\n",
    "from pfl.aggregate.simulate import SimulatedBackend\n",
    "\n",
    "\n",
    "# model_train_params = NNTrainHyperParams(\n",
    "#     local_learning_rate=local_learning_rate,\n",
    "#     local_num_epochs=local_num_epochs,\n",
    "#     local_batch_size=local_batch_size)\n",
    "\n",
    "# # Do full-batch evaluation to run faster.\n",
    "# model_eval_params = NNEvalHyperParams(local_batch_size=None)\n",
    "\n",
    "# algorithm_params = NNAlgorithmParams(\n",
    "#     central_num_iterations=central_num_iterations,\n",
    "#     evaluation_frequency=evaluation_frequency,\n",
    "#     train_cohort_size=cohort_size,\n",
    "#     val_cohort_size=1)\n",
    "\n",
    "# pfl_callbacks = [CentralEvaluationCallback(central_data, model_eval_params, evaluation_frequency),\n",
    "#                   AggregateMetricsToDisk(output_path='pfl_training_metrics/metrics.csv')] # Save the metrics to a CSV file\n",
    "\n",
    "# # Add the privacy mechanism to the training process\n",
    "# # If you do not want to add privacy, simply leave 'postprocessors' empty\n",
    "# postprocessors = [rdp_central_privacy]\n",
    "\n",
    "# pfl_simulated_backend = SimulatedBackend(\n",
    "#     training_data=federated_dataset,\n",
    "#     val_data=val_federated_dataset,\n",
    "#     postprocessors=postprocessors\n",
    "# )\n",
    "\n",
    "\n",
    "algorithm = FederatedAveraging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pfl_model = algorithm.run(\n",
    "#     backend=pfl_simulated_backend,\n",
    "#     model=model,\n",
    "#     algorithm_params=algorithm_params,\n",
    "#     model_train_params=model_train_params,\n",
    "#     model_eval_params=model_eval_params,\n",
    "#     callbacks=pfl_callbacks,\n",
    "#     send_metrics_to_platform=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# metrics = pd.read_csv('pfl_training_metrics/metrics.csv')\n",
    "# metrics = metrics.dropna(subset=['Central val | accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(metrics['central_iteration'], metrics['Central val | accuracy'], label='Central Validation Accuracy')\n",
    "# plt.xlabel('Central Iteration')\n",
    "# plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation\n",
    "\n",
    "The sections \"Setting up the model\", \"Run the training\" and \"Plot the metrics\" should demonstrate a single instance of a Federated Learning training.\n",
    "\n",
    "However, we as researchers wish to run a particular training several times and take the average of the accuracy of each training instance. The following code should do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Gaussian DP mechanisms for central DP guarantees using three different methods\n",
    "\n",
    "n_clients = 8                                  # Number of clients\n",
    "n_samples = len(trainset)           # Number of training samples\n",
    "user_dataset_size = int(n_samples / n_clients) # Size of each user's dataset\n",
    "clipping_bound = 0.5                           # Clipping bound for DP\n",
    "epsilon = 1                                    # Epsilon parameter for DP\n",
    "delta = 1e-8                                   # Delta parameter for DP\n",
    "is_central = True                              # Probably delete this line in the future, as it is not used anywhere else in the code.\n",
    "\n",
    "# Other parameters for the training process\n",
    "\n",
    "local_learning_rate = 0.1                    # Learning rate for local training\n",
    "local_num_epochs = 4                           # Number of local epochs for each client\n",
    "local_batch_size = 32                          # Batch size for local training\n",
    "central_num_iterations = 100                    # Number of central iterations\n",
    "evaluation_frequency = 1                       # How often to evaluate the model (in terms of central iterations)\n",
    "cohort_size = 8                                # Number of clients in the cohort\n",
    "sampling_probability = cohort_size / n_clients # Probability of a client being selected as cohort for a given iteration: cohort_size / n_clients\n",
    "\n",
    "# Choose the number of FL training instances\n",
    "no_of_training : int = 5\n",
    "\n",
    "# Choose a name for the training metrics file\n",
    "training_name = \"pfl_centralized_fixed_metrics_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pfl_model():\n",
    "    model = create_pytorch_model()\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    return PyTorchModel(model,\n",
    "                        local_optimizer_create=torch.optim.SGD,\n",
    "                        central_optimizer=torch.optim.SGD(params, lr=local_learning_rate, weight_decay=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "params = [p for p in pytorch_model.parameters() if p.requires_grad]\n",
    "\n",
    "model = PyTorchModel(pytorch_model, \n",
    "                     local_optimizer_create=torch.optim.Adam,\n",
    "                     central_optimizer=torch.optim.Adam(params, lr=local_learning_rate, weight_decay=1e-5))\n",
    "'''\n",
    "                     \n",
    "model_train_params = NNTrainHyperParams(\n",
    "    local_learning_rate=local_learning_rate,\n",
    "    local_num_epochs=local_num_epochs,\n",
    "    local_batch_size=local_batch_size)\n",
    "\n",
    "# Do full-batch evaluation to run faster.\n",
    "model_eval_params = NNEvalHyperParams(local_batch_size=None)\n",
    "\n",
    "algorithm_params = NNAlgorithmParams(\n",
    "    central_num_iterations=central_num_iterations,\n",
    "    evaluation_frequency=evaluation_frequency,\n",
    "    train_cohort_size=cohort_size,\n",
    "    val_cohort_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_experiements(no_of_training: int, training_name: str, n_clients: int, user_dataset_size: int, \n",
    "                     is_random_dataset: bool = True, postprocessors: list = []):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        no_of_training (int): Number of training instances to run.\n",
    "        training_name (str): Name for the training metrics file.\n",
    "        n_clients (int): Number of clients.\n",
    "        user_dataset_size (int): Size of each user's dataset.\n",
    "        is_random_dataset (bool): Whether to use a random dataset or a fixed dataset.\n",
    "        postprocessors (list): List of postprocessors to apply during training. This is used to add the privacy mechanism. By default, there is no privacy mechanism.\n",
    "    \"\"\"\n",
    "\n",
    "    create_federated_dataset = create_random_federated_dataset if is_random_dataset else create_fixed_federated_dataset\n",
    "\n",
    "    for i in range(no_of_training):\n",
    "        # Initialize the model for each training instance\n",
    "        model = create_pfl_model()\n",
    "\n",
    "        # Create a new federated dataset\n",
    "        # Either use create_fixed_federated_dataset or create_random_federated_dataset depending on your needs\n",
    "        federated_dataset = create_federated_dataset(\n",
    "            n_clients=n_clients,\n",
    "            user_dataset_size=user_dataset_size,\n",
    "            train_all_features=train_features,\n",
    "            train_all_labels=train_labels\n",
    "        )\n",
    "\n",
    "        # Create a new validation federated dataset\n",
    "        val_federated_dataset = ValidationFederatedDataset(Dataset(raw_data=[val_features, val_labels]))\n",
    "\n",
    "        pfl_callbacks = [CentralEvaluationCallback(central_data, model_eval_params, evaluation_frequency),\n",
    "                    AggregateMetricsToDisk(output_path=f'experimental_results/{training_name}/metrics_{i}.csv')] # Save the metrics to a CSV file\n",
    "\n",
    "\n",
    "        # Run the training process again with the new datasets\n",
    "        pfl_simulated_backend = SimulatedBackend(\n",
    "            training_data=federated_dataset,\n",
    "            val_data=val_federated_dataset,\n",
    "            postprocessors=postprocessors\n",
    "        )\n",
    "\n",
    "        algorithm.run(\n",
    "            backend=pfl_simulated_backend,\n",
    "            model=model,\n",
    "            algorithm_params=algorithm_params,\n",
    "            model_train_params=model_train_params,\n",
    "            model_eval_params=model_eval_params,\n",
    "            callbacks=pfl_callbacks,\n",
    "            send_metrics_to_platform=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is used to define the privacy mechanism for the training process\n",
    "\n",
    "rdp_accountant = RDPPrivacyAccountant(\n",
    "    num_compositions=central_num_iterations,\n",
    "    sampling_probability=sampling_probability,\n",
    "    mechanism='gaussian',\n",
    "    epsilon=epsilon,\n",
    "    delta=delta\n",
    "    #noise_parameter = 2 * np.log(1.25 / delta) * 1/(epsilon**2)  # Relative noise standard deviation\n",
    ")\n",
    "\n",
    "# As I understand, this is the local noise mechanism that will be applied to the local model updates\n",
    "rdp_gaussian_noise_mechanism = GaussianMechanism.from_privacy_accountant(\n",
    "    accountant=rdp_accountant, clipping_bound=clipping_bound)\n",
    "\n",
    "# This line wraps the noise mechanism with CentrallyAppliedPrivacyMechanism to make it a central privacy mechanism\n",
    "rdp_central_privacy = CentrallyAppliedPrivacyMechanism(rdp_gaussian_noise_mechanism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of model parameters before: -5.729608327150345\n",
      "Sum of model parameters after: -2.9547024369239807\n",
      "Metrics at iteration 0 ():\n",
      "    Central val | loss                                : 0.000230730938911438\n",
      "    Central val | accuracy                            : 0.1027\n",
      "    Central val | number of data points               : 10000\n",
      "    Train population | number of devices              : 8\n",
      "    Train population | number of data points          : 7500.0\n",
      "    Train population | loss before local training     : 0.00030765488942464193\n",
      "    Train population | accuracy before local training : 0.10331666666666667\n",
      "    Train population | loss after local training      : 8.193655560413996e-06\n",
      "    Train population | accuracy after local training  : 0.9806333333333334\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 61610\n",
      "    Val population | number of devices                : 1\n",
      "    Val population | number of data points            : 10000.0\n",
      "    Val population | loss before local training       : 0.000230730938911438\n",
      "    Val population | accuracy before local training   : 0.1027\n",
      "    Learning rate                                     : 0.1\n",
      "Sum of model parameters before: -2.9547024369239807\n",
      "Sum of model parameters after: 0.27188846468925476\n",
      "Metrics at iteration 1 ():\n",
      "    Train population | number of devices              : 8\n",
      "    Train population | number of data points          : 7500.0\n",
      "    Train population | loss before local training     : 0.0003044287363688151\n",
      "    Train population | accuracy before local training : 0.19453333333333334\n",
      "    Train population | loss after local training      : 6.047914798061053e-06\n",
      "    Train population | accuracy after local training  : 0.9863666666666666\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 61610\n",
      "    Val population | number of devices                : 1\n",
      "    Val population | number of data points            : 10000.0\n",
      "    Val population | loss before local training       : 0.00022822582721710206\n",
      "    Val population | accuracy before local training   : 0.1887\n",
      "    Learning rate                                     : 0.1\n",
      "    Central val | accuracy                            : 0.478\n",
      "    Central val | loss                                : 0.00022115089893341066\n",
      "    Central val | number of data points               : 10000.0\n",
      "Sum of model parameters before: 0.27188846468925476\n",
      "Sum of model parameters after: 3.408918797969818\n",
      "Metrics at iteration 2 ():\n",
      "    Train population | number of devices              : 8\n",
      "    Train population | number of data points          : 7500.0\n",
      "    Train population | loss before local training     : 0.00029522250493367515\n",
      "    Train population | accuracy before local training : 0.4707\n",
      "    Train population | loss after local training      : 5.78872337937355e-06\n",
      "    Train population | accuracy after local training  : 0.9858666666666667\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 61610\n",
      "    Val population | number of devices                : 1\n",
      "    Val population | number of data points            : 10000.0\n",
      "    Val population | loss before local training       : 0.00022115089893341066\n",
      "    Val population | accuracy before local training   : 0.478\n",
      "    Learning rate                                     : 0.1\n",
      "    Central val | accuracy                            : 0.8469\n",
      "    Central val | loss                                : 0.0002084683895111084\n",
      "    Central val | number of data points               : 10000.0\n",
      "Sum of model parameters before: 3.408918797969818\n",
      "Sum of model parameters after: 6.9685188382864\n",
      "Metrics at iteration 3 ():\n",
      "    Train population | number of devices              : 8\n",
      "    Train population | number of data points          : 7500.0\n",
      "    Train population | loss before local training     : 0.0002785795291264852\n",
      "    Train population | accuracy before local training : 0.8354166666666667\n",
      "    Train population | loss after local training      : 5.041985524197419e-06\n",
      "    Train population | accuracy after local training  : 0.9878333333333333\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 61610\n",
      "    Val population | number of devices                : 1\n",
      "    Val population | number of data points            : 10000.0\n",
      "    Val population | loss before local training       : 0.0002084683895111084\n",
      "    Val population | accuracy before local training   : 0.8469\n",
      "    Learning rate                                     : 0.1\n",
      "    Central val | accuracy                            : 0.9375\n",
      "    Central val | loss                                : 0.00018986302614212036\n",
      "    Central val | number of data points               : 10000.0\n",
      "Sum of model parameters before: 6.9685188382864\n",
      "Sum of model parameters after: 10.552737660706043\n",
      "Metrics at iteration 4 ():\n",
      "    Train population | number of devices              : 8\n",
      "    Train population | number of data points          : 7500.0\n",
      "    Train population | loss before local training     : 0.00025414708852767947\n",
      "    Train population | accuracy before local training : 0.9331166666666667\n",
      "    Train population | loss after local training      : 5.366328048209349e-06\n",
      "    Train population | accuracy after local training  : 0.9874\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 61610\n",
      "    Val population | number of devices                : 1\n",
      "    Val population | number of data points            : 10000.0\n",
      "    Val population | loss before local training       : 0.00018986302614212036\n",
      "    Val population | accuracy before local training   : 0.9375\n",
      "    Learning rate                                     : 0.1\n",
      "    Central val | accuracy                            : 0.9616\n",
      "    Central val | loss                                : 0.00016629446744918823\n",
      "    Central val | number of data points               : 10000.0\n",
      "Sum of model parameters before: 10.552737660706043\n",
      "Sum of model parameters after: 13.886104699224234\n",
      "Metrics at iteration 5 ():\n",
      "    Train population | number of devices              : 8\n",
      "    Train population | number of data points          : 7500.0\n",
      "    Train population | loss before local training     : 0.00022304280002911886\n",
      "    Train population | accuracy before local training : 0.9603833333333334\n",
      "    Train population | loss after local training      : 4.476853304853042e-06\n",
      "    Train population | accuracy after local training  : 0.9894\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 61610\n",
      "    Val population | number of devices                : 1\n",
      "    Val population | number of data points            : 10000.0\n",
      "    Val population | loss before local training       : 0.00016629446744918823\n",
      "    Val population | accuracy before local training   : 0.9616\n",
      "    Learning rate                                     : 0.1\n",
      "    Central val | accuracy                            : 0.9699\n",
      "    Central val | loss                                : 0.00013899123668670654\n",
      "    Central val | number of data points               : 10000.0\n",
      "Sum of model parameters before: 13.886104699224234\n",
      "Sum of model parameters after: 17.3078106418252\n",
      "Metrics at iteration 6 ():\n",
      "    Train population | number of devices              : 8\n",
      "    Train population | number of data points          : 7500.0\n",
      "    Train population | loss before local training     : 0.0001869272530078888\n",
      "    Train population | accuracy before local training : 0.9683833333333334\n",
      "    Train population | loss after local training      : 4.067578352987767e-06\n",
      "    Train population | accuracy after local training  : 0.9906833333333334\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 61610\n",
      "    Val population | number of devices                : 1\n",
      "    Val population | number of data points            : 10000.0\n",
      "    Val population | loss before local training       : 0.00013899123668670654\n",
      "    Val population | accuracy before local training   : 0.9699\n",
      "    Learning rate                                     : 0.1\n",
      "    Central val | accuracy                            : 0.9748\n",
      "    Central val | loss                                : 0.00011014908552169799\n",
      "    Central val | number of data points               : 10000.0\n",
      "Sum of model parameters before: 17.3078106418252\n",
      "Sum of model parameters after: 20.183795779943466\n",
      "Metrics at iteration 7 ():\n",
      "    Train population | number of devices              : 8\n",
      "    Train population | number of data points          : 7500.0\n",
      "    Train population | loss before local training     : 0.00014869225223859152\n",
      "    Train population | accuracy before local training : 0.97395\n",
      "    Train population | loss after local training      : 4.090613654504219e-06\n",
      "    Train population | accuracy after local training  : 0.99105\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 61610\n",
      "    Val population | number of devices                : 1\n",
      "    Val population | number of data points            : 10000.0\n",
      "    Val population | loss before local training       : 0.00011014908552169799\n",
      "    Val population | accuracy before local training   : 0.9748\n",
      "    Learning rate                                     : 0.1\n",
      "    Central val | accuracy                            : 0.9775\n",
      "    Central val | loss                                : 8.287165760993958e-05\n",
      "    Central val | number of data points               : 10000.0\n",
      "Sum of model parameters before: 20.183795779943466\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrun_experiements\u001b[49m\u001b[43m(\u001b[49m\u001b[43mno_of_training\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_clients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_dataset_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mis_random_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocessors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[44], line 43\u001b[0m, in \u001b[0;36mrun_experiements\u001b[1;34m(no_of_training, training_name, n_clients, user_dataset_size, is_random_dataset, postprocessors)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Run the training process again with the new datasets\u001b[39;00m\n\u001b[0;32m     37\u001b[0m pfl_simulated_backend \u001b[38;5;241m=\u001b[39m SimulatedBackend(\n\u001b[0;32m     38\u001b[0m     training_data\u001b[38;5;241m=\u001b[39mfederated_dataset,\n\u001b[0;32m     39\u001b[0m     val_data\u001b[38;5;241m=\u001b[39mval_federated_dataset,\n\u001b[0;32m     40\u001b[0m     postprocessors\u001b[38;5;241m=\u001b[39mpostprocessors\n\u001b[0;32m     41\u001b[0m )\n\u001b[1;32m---> 43\u001b[0m \u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpfl_simulated_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgorithm_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgorithm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_train_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_train_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_eval_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_eval_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpfl_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43msend_metrics_to_platform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AVN\\anaconda3\\envs\\masters\\lib\\site-packages\\pfl\\algorithm\\base.py:291\u001b[0m, in \u001b[0;36mFederatedAlgorithm.run\u001b[1;34m(self, algorithm_params, backend, model, model_train_params, model_eval_params, callbacks, send_metrics_to_platform)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSum of model parameters before:\u001b[39m\u001b[38;5;124m\"\u001b[39m, param_sum)\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# Step 2\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# Get aggregated model updates and\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# metrics from the requested queries.\u001b[39;00m\n\u001b[0;32m    290\u001b[0m results: List[Tuple[StatisticsType,\n\u001b[1;32m--> 291\u001b[0m                     Metrics]] \u001b[38;5;241m=\u001b[39m \u001b[43malgorithm_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_train_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcentral_contexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;66;03m# Step 3\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# For each query result, accumulate metrics and\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;66;03m# let model handle statistics result if query had any.\u001b[39;00m\n\u001b[0;32m    297\u001b[0m stats_context_pairs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\AVN\\anaconda3\\envs\\masters\\lib\\site-packages\\pfl\\algorithm\\algorithm_utils.py:63\u001b[0m, in \u001b[0;36mrun_train_eval\u001b[1;34m(algorithm, backend, model, central_contexts)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_all_iterations\u001b[39m():\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39m[\n\u001b[0;32m     59\u001b[0m         run_iteration(central_context, initial_sleep\u001b[38;5;241m=\u001b[39mindex \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m index, central_context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(central_contexts)\n\u001b[0;32m     61\u001b[0m     ])\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_event_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_all_iterations\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nest_asyncio.py:133\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    130\u001b[0m curr_task \u001b[38;5;241m=\u001b[39m curr_tasks\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 133\u001b[0m     \u001b[43mhandle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;66;03m# restore the current task\u001b[39;00m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m curr_task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\AVN\\anaconda3\\envs\\masters\\lib\\asyncio\\events.py:80\u001b[0m, in \u001b[0;36mHandle._run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 80\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mSystemExit\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[0;32m     82\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\AVN\\anaconda3\\envs\\masters\\lib\\asyncio\\tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[1;32mc:\\Users\\AVN\\anaconda3\\envs\\masters\\lib\\site-packages\\pfl\\algorithm\\algorithm_utils.py:52\u001b[0m, in \u001b[0;36mrun_train_eval.<locals>.run_iteration\u001b[1;34m(central_context, initial_sleep)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m initial_sleep:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# This is needed to ensure that the iteration with no sleep\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m# is ordered to start before other iterations with initial sleep.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# Which is needed for synchronous worker communication in\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# distributed simulations.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m backend\u001b[38;5;241m.\u001b[39masync_gather_results(\n\u001b[0;32m     53\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     54\u001b[0m     training_algorithm\u001b[38;5;241m=\u001b[39malgorithm,\n\u001b[0;32m     55\u001b[0m     central_context\u001b[38;5;241m=\u001b[39mcentral_context)\n",
      "File \u001b[1;32mc:\\Users\\AVN\\anaconda3\\envs\\masters\\lib\\site-packages\\pfl\\aggregate\\simulate.py:152\u001b[0m, in \u001b[0;36mSimulatedBackend.async_gather_results\u001b[1;34m(self, model, training_algorithm, central_context)\u001b[0m\n\u001b[0;32m    146\u001b[0m server_statistics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m user_dataset, local_seed \u001b[38;5;129;01min\u001b[39;00m selected_dataset\u001b[38;5;241m.\u001b[39mget_cohort(\n\u001b[0;32m    149\u001b[0m         cohort_size):\n\u001b[0;32m    151\u001b[0m     user_statistics, metrics_one_user \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 152\u001b[0m         \u001b[43mtraining_algorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulate_one_user\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mcentral_context\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    155\u001b[0m     user_context \u001b[38;5;241m=\u001b[39m UserContext(num_datapoints\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(user_dataset),\n\u001b[0;32m    156\u001b[0m                                seed\u001b[38;5;241m=\u001b[39mlocal_seed,\n\u001b[0;32m    157\u001b[0m                                user_id\u001b[38;5;241m=\u001b[39muser_dataset\u001b[38;5;241m.\u001b[39muser_id,\n\u001b[0;32m    158\u001b[0m                                metrics\u001b[38;5;241m=\u001b[39mmetrics_one_user)\n\u001b[0;32m    160\u001b[0m     num_total_datapoints \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m Weighted\u001b[38;5;241m.\u001b[39mfrom_unweighted(\n\u001b[0;32m    161\u001b[0m         user_context\u001b[38;5;241m.\u001b[39mnum_datapoints)\n",
      "File \u001b[1;32mc:\\Users\\AVN\\anaconda3\\envs\\masters\\lib\\site-packages\\pfl\\algorithm\\base.py:469\u001b[0m, in \u001b[0;36mFederatedNNAlgorithm.simulate_one_user\u001b[1;34m(self, model, user_dataset, central_context)\u001b[0m\n\u001b[0;32m    463\u001b[0m     metrics \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(user_dataset,\n\u001b[0;32m    464\u001b[0m                               initial_metrics_format_fn,\n\u001b[0;32m    465\u001b[0m                               central_context\u001b[38;5;241m.\u001b[39mmodel_eval_params)\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_model_state \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_parameters(\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_model_state)\n\u001b[1;32m--> 469\u001b[0m statistics, train_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_user\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initial_model_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcentral_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    472\u001b[0m metrics \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m train_metrics\n\u001b[0;32m    474\u001b[0m \u001b[38;5;66;03m# Evaluate after local training.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\AVN\\anaconda3\\envs\\masters\\lib\\site-packages\\pfl\\algorithm\\federated_averaging.py:45\u001b[0m, in \u001b[0;36mFederatedAveraging.train_one_user\u001b[1;34m(self, initial_model_state, model, user_dataset, central_context)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain_one_user\u001b[39m(\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m, initial_model_state: WeightedStatistics,\n\u001b[0;32m     41\u001b[0m     model: StatefulModelType, user_dataset: AbstractDatasetType,\n\u001b[0;32m     42\u001b[0m     central_context: FedAvgCentralContextType\n\u001b[0;32m     43\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[WeightedStatistics, Metrics]:\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# Local training loop\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     \u001b[43mbridges\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msgd_bridge\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_sgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mcentral_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_train_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     training_statistics \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_model_difference(initial_model_state)\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# Don't reset model, will be used for evaluation after local training.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\AVN\\anaconda3\\envs\\masters\\lib\\site-packages\\pfl\\internal\\bridge\\pytorch\\sgd.py:41\u001b[0m, in \u001b[0;36mPyTorchSGDBridge.do_sgd\u001b[1;34m(model, user_dataset, train_params)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdo_sgd\u001b[39m(model: PyTorchModel, user_dataset: AbstractDatasetType,\n\u001b[0;32m     40\u001b[0m            train_params: NNTrainHyperParams) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 41\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_multiple_epochs_of\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m                                \u001b[49m\u001b[43m_sgd_train_step\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AVN\\anaconda3\\envs\\masters\\lib\\site-packages\\pfl\\model\\pytorch.py:369\u001b[0m, in \u001b[0;36mPyTorchModel.do_multiple_epochs_of\u001b[1;34m(self, user_dataset, train_params, train_step_fn, **kwargs)\u001b[0m\n\u001b[0;32m    367\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    368\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_batch(batch)\n\u001b[1;32m--> 369\u001b[0m         train_step_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model, local_optimizer, batch,\n\u001b[0;32m    370\u001b[0m                       user_dataset\u001b[38;5;241m.\u001b[39mtrain_kwargs, train_step_args,\n\u001b[0;32m    371\u001b[0m                       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LocalResultMetaData(num_steps\u001b[38;5;241m=\u001b[39mlocal_num_steps)\n",
      "File \u001b[1;32mc:\\Users\\AVN\\anaconda3\\envs\\masters\\lib\\site-packages\\pfl\\internal\\bridge\\pytorch\\sgd.py:14\u001b[0m, in \u001b[0;36m_sgd_train_step\u001b[1;34m(pytorch_model, local_optimizer, raw_data, train_kwargs, train_step_args)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_sgd_train_step\u001b[39m(pytorch_model, local_optimizer, raw_data, train_kwargs,\n\u001b[0;32m     13\u001b[0m                     train_step_args):\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m train_step_args\u001b[38;5;241m.\u001b[39mamp_context:\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(raw_data, Dict):\n\u001b[0;32m     16\u001b[0m             loss \u001b[38;5;241m=\u001b[39m pytorch_model\u001b[38;5;241m.\u001b[39mloss(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mraw_data, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrain_kwargs})\n",
      "File \u001b[1;32mc:\\Users\\AVN\\anaconda3\\envs\\masters\\lib\\contextlib.py:738\u001b[0m, in \u001b[0;36mnullcontext.__exit__\u001b[1;34m(self, *excinfo)\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menter_result\n\u001b[1;32m--> 738\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mexcinfo):\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    741\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "run_experiements(no_of_training, training_name, n_clients, user_dataset_size, \n",
    "                 is_random_dataset=False, postprocessors=[])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
