{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Both Jupyter and `pfl` use async. `nest_asyncio` allows `pfl` to run inside the notebook \n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# append the root directory to your paths to be able to reach the examples.  \n",
    "torch.random.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# Always import the `pfl` model first before initializing any `pfl` components to let `pfl` know which Deep Learning framework you will use.\n",
    "import multiprocessing\n",
    "# Set multiprocessing start method to \"spawn\" instead of forkserver (which is the default)\n",
    "# That is because forkserver does not work on Windows, but spawn does.\n",
    "def init_multiprocessing():\n",
    "    try:\n",
    "        multiprocessing.set_start_method(\"spawn\", force=True)  # Forces \"spawn\"\n",
    "    except RuntimeError:\n",
    "        pass  # Ignore if it's already set\n",
    "\n",
    "init_multiprocessing()\n",
    "\n",
    "from pfl.model.pytorch import PyTorchModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the DP mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Gaussian DP mechanisms for central DP guarantees using three different methods\n",
    "\n",
    "clipping_bound = 0.5\n",
    "epsilon = 2\n",
    "delta = 1e-8\n",
    "cohort_size = 50\n",
    "num_epochs = 100 # For DP\n",
    "central_num_iterations = 10\n",
    "sampling_probability = 1e-4\n",
    "is_central = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.gautamkamath.com/CS860notes/lec5.pdf\n",
    "\n",
    "Definition 5 on page 3 gives the definition of the parameters for Gaussian distribution where we sample noise for DP.\n",
    "\n",
    "$\\Delta_2^2$ is not included when defining `relative_noise_stddev`, because the clipping bound (which I understand to be $\\Delta_2^2$ and also is $\\Delta_2^{(f)}$ from definition 3 on page 3) is multiplied on `relative_noise_stddev`, when sampling the noise for DP via `GaussianMechanism.add_noise()`\n",
    "\n",
    "Source: https://apple.github.io/pfl-research/reference/privacy.html#pfl.privacy.gaussian_mechanism.GaussianMechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "relative_noise_stddev = 2 * math.log(1.25 / delta) * 1/(epsilon**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a Gaussian DP mechanism using the PLD privacy accountant\n",
    "# WARNING: it takes a while for the gaussian_moments_accountant mechanism to be instantiated\n",
    "\n",
    "from pfl.privacy import (PLDPrivacyAccountant, CentrallyAppliedPrivacyMechanism, GaussianMechanism, LocalPrivacyMechanism)\n",
    "\n",
    "# define the PLD privacy accountant, which will use the Gaussian noise mechanism\n",
    "pld_accountant = PLDPrivacyAccountant(\n",
    "    num_compositions=num_epochs,\n",
    "    sampling_probability=sampling_probability,\n",
    "    mechanism='gaussian',\n",
    "    epsilon=epsilon,\n",
    "    delta=delta)\n",
    "\n",
    "# instantiate a Gaussian noise mechanism using the privacy accountant\n",
    "pld_gaussian_noise_mechanism = GaussianMechanism.from_privacy_accountant(\n",
    "    accountant=pld_accountant, clipping_bound=clipping_bound)\n",
    "\n",
    "# wrap the noise mechanism with CentrallyAppliedPrivacyMechanism to make it a central privacy mechanism\n",
    "pld_central_privacy = CentrallyAppliedPrivacyMechanism(pld_gaussian_noise_mechanism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Local DP mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://apple.github.io/pfl-research/reference/privacy.html#pfl.privacy.gaussian_mechanism.GaussianMechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfl.metrics import Metrics\n",
    "\n",
    "class LocallyAppliedPrivacyMechanism(LocalPrivacyMechanism):\n",
    "\n",
    "    def privatize(self, statistics, name_formatting_fn=..., seed = None):\n",
    "        # TODO: Implement actual privatization for local DP\n",
    "        # TODO: Sample some noise (Gaussian: parameters depend on privacy parameters)\n",
    "        gaussian_mechanism = GaussianMechanism(clipping_bound=clipping_bound, \n",
    "                                               relative_noise_stddev=relative_noise_stddev)\n",
    "        noisy_statistics, metrics = gaussian_mechanism.add_noise(statistics=statistics, \n",
    "                                     cohort_size=cohort_size, \n",
    "                                     name_formatting_fn=name_formatting_fn)\n",
    "        # TODO: Add the noise to statistics then send it\n",
    "        return noisy_statistics, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, Optional\n",
    "from pfl.metrics import Weighted\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "pytorch_model = Net()\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def loss(inputs: torch.Tensor, targets: torch.Tensor, eval: bool = False) -> torch.Tensor:\n",
    "    pytorch_model.eval() if eval else pytorch_model.train()\n",
    "    return loss_fn(pytorch_model(inputs), targets)\n",
    "\n",
    "\"\"\"\n",
    "@torch.no_grad()\n",
    "def metrics(inputs: torch.Tensor,\n",
    "             targets: torch.Tensor,\n",
    "             eval: bool = True) -> Dict[str, Weighted]:\n",
    "    pytorch_model.eval() if eval else pytorch_model.train()\n",
    "    prediction = pytorch_model(inputs)\n",
    "    logits = torch.argmax(prediction, dim=1)\n",
    "    num_samples = len(inputs)\n",
    "    num_predictions = targets.numel()\n",
    "    correct = torch.sum(torch.eq((logits > 0.0).float(), targets))\n",
    "\n",
    "    loss = loss_fn(prediction, targets).item()\n",
    "    return {\n",
    "        \"loss\": Weighted(loss, num_samples),\n",
    "        \"accuracy\": Weighted(correct, num_predictions)\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def metrics(inputs: torch.Tensor,\n",
    "             targets: torch.Tensor,\n",
    "             eval: bool = True) -> Dict[str, Weighted]:\n",
    "    pytorch_model.eval() if eval else pytorch_model.train()\n",
    "    prediction = pytorch_model(inputs)\n",
    "    logits = torch.argmax(prediction, dim=1)\n",
    "    num_samples = len(inputs)\n",
    "    num_predictions = targets.numel()\n",
    "    correct = torch.sum(torch.eq((logits > 0.0).float(), targets))\n",
    "\n",
    "    loss = loss_fn(prediction, targets).item()\n",
    "    return {\n",
    "        \"loss\": Weighted(loss, num_samples),\n",
    "        \"accuracy\": Weighted(correct, num_samples)\n",
    "    }\n",
    "\n",
    "\n",
    "pytorch_model.loss = loss\n",
    "pytorch_model.metrics = metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfl.data.dataset import Dataset\n",
    "import sys\n",
    "\n",
    "sys.path.append('../flower/flower_normal')\n",
    "from centralized import load_data\n",
    "train_set, test_set = load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training dataloader into features and labels\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "for features, labels in train_set:\n",
    "    all_features.append(features)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "# Convert to tensors if needed\n",
    "all_features = torch.cat(all_features, dim=0)\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "central_data = Dataset([all_features, all_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 3, 32, 32])\n",
      "torch.Size([50000])\n"
     ]
    }
   ],
   "source": [
    "print(all_features.shape)\n",
    "print(all_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the validation dataloader into features and labels\n",
    "val_all_features = []\n",
    "val_all_labels = []\n",
    "\n",
    "for features, labels in test_set:\n",
    "    val_all_features.append(features)\n",
    "    val_all_labels.append(labels)\n",
    "\n",
    "# Convert to tensors if needed\n",
    "val_all_features = torch.cat(val_all_features, dim=0)\n",
    "val_all_labels = torch.cat(val_all_labels, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 3, 32, 32])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(val_all_features.shape)\n",
    "print(val_all_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "n_samples = len(train_set.dataset)\n",
    "print(n_samples)\n",
    "val_n_samples = len(test_set.dataset)\n",
    "print(val_n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating artificial federated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfl.data import ArtificialFederatedDataset, get_data_sampler\n",
    "\n",
    "# Create data sampler to sample each artificial user dataset as a random subset of the original dataset\n",
    "data_sampler = get_data_sampler(sample_type=\"minimize_reuse\", max_bound=n_samples)\n",
    "\n",
    "# Create an artificial federated dataset where each user dataset has constant size such that there are 10 users to distribute among\n",
    "sample_dataset_len = lambda: int(n_samples/10)\n",
    "federated_dataset = ArtificialFederatedDataset.from_slices(\n",
    "    data=[all_features, all_labels], \n",
    "    data_sampler=data_sampler,\n",
    "    sample_dataset_len=sample_dataset_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_sampler = get_data_sampler(sample_type=\"minimize_reuse\", max_bound=val_n_samples)\n",
    "\n",
    "# Create an artificial federated dataset where each user dataset has constant size such that there are of 10 users to distribute among\n",
    "val_sample_dataset_len = lambda: int(val_n_samples/10)\n",
    "val_federated_dataset = ArtificialFederatedDataset.from_slices(\n",
    "    data=[val_all_features, val_all_labels],\n",
    "    data_sampler=val_data_sampler,\n",
    "    sample_dataset_len=val_sample_dataset_len\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a federated dataset\n",
    "\n",
    "For the purpose of being able to control the actual number of users, which `ArtificialFederatedDataset` does not allow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Reflection on designing `FederatedDataset`</strong>\n",
    "\n",
    "How many datapoints should each user have?\n",
    "- Every user has 50000/100 = 500 datapoints (easy approach)\n",
    "- Alternatively, do some randomization on how many datapoints each user has (harder approach)\n",
    "\n",
    "Should we use the exact same federated dataset in every FL experiment?\n",
    "- The datapoint will be allocated to the same user in every experiment. This means that user 0 will have the exact same datapoints. User 1 will also have the exact same datapoints and so on... for every single time we run our experiment. This means our federated dataset will be exactly the same every time we run our experiment.\n",
    "\n",
    "We decided to assume that all users have equally many datapoints and they will each possess the same datapoints every time we run our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfl.data import FederatedDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I must first decide how many users there are and then the method to how the data is distributed among the users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "n_users = 100\n",
    "\n",
    "\n",
    "user_id_to_data = {}\n",
    "\n",
    "for i in range(n_users):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_dataset = FederatedDataset.from_slices(\n",
    "    data=user_id_to_data\n",
    "    user_sampler=user_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in pytorch_model.parameters() if p.requires_grad]\n",
    "\n",
    "model = PyTorchModel(pytorch_model, \n",
    "                     local_optimizer_create=torch.optim.SGD,\n",
    "                     central_optimizer=torch.optim.SGD(params, 0.1, momentum=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pfl.algorithm import FederatedAveraging, NNAlgorithmParams\n",
    "from pfl.callback import CentralEvaluationCallback, AggregateMetricsToDisk\n",
    "from pfl.hyperparam import NNTrainHyperParams, NNEvalHyperParams\n",
    "from pfl.aggregate.simulate import SimulatedBackend\n",
    "\n",
    "\n",
    "model_train_params = NNTrainHyperParams(\n",
    "    local_learning_rate=0.1,\n",
    "    local_num_epochs=5,\n",
    "    local_batch_size=32)\n",
    "\n",
    "# Do full-batch evaluation to run faster.\n",
    "model_eval_params = NNEvalHyperParams(local_batch_size=None)\n",
    "\n",
    "evaluation_frequency = 1\n",
    "algorithm_params = NNAlgorithmParams(\n",
    "    central_num_iterations=central_num_iterations,\n",
    "    evaluation_frequency=evaluation_frequency,\n",
    "    train_cohort_size=cohort_size,\n",
    "    val_cohort_size=2)\n",
    "\n",
    "pfl_callbacks = [CentralEvaluationCallback(central_data, model_eval_params, evaluation_frequency), AggregateMetricsToDisk(output_path='pfl_training_metrics/metrics.csv')]\n",
    "\n",
    "#postprocessors = [LocallyAppliedPrivacyMechanism()]\n",
    "postprocessors = []\n",
    "\n",
    "pfl_simulated_backend = SimulatedBackend(\n",
    "    training_data=federated_dataset,\n",
    "    val_data=val_federated_dataset,\n",
    "    postprocessors=postprocessors\n",
    ")\n",
    "\n",
    "\n",
    "algorithm = FederatedAveraging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics at iteration 0 ():\n",
      "    Central val | loss                                : 4.6076517105102536e-05\n",
      "    Central val | accuracy                            : 0.1\n",
      "    Central val | number of data points               : 50000\n",
      "    Train population | number of devices              : 50\n",
      "    Train population | number of data points          : 5000.0\n",
      "    Train population | loss before local training     : 0.0004607900915145874\n",
      "    Train population | accuracy before local training : 0.100112\n",
      "    Train population | loss after local training      : 0.00034826629400253295\n",
      "    Train population | accuracy after local training  : 0.137248\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 62006\n",
      "    Val population | number of devices                : 2\n",
      "    Val population | number of data points            : 1000.0\n",
      "    Val population | loss before local training       : 0.0023041212558746337\n",
      "    Val population | accuracy before local training   : 0.1085\n",
      "    Learning rate                                     : 0.1\n",
      "Metrics at iteration 1 ():\n",
      "    Train population | number of devices              : 50\n",
      "    Train population | number of data points          : 5000.0\n",
      "    Train population | loss before local training     : 0.0004600140256881714\n",
      "    Train population | accuracy before local training : 0.100016\n",
      "    Train population | loss after local training      : 0.00031893779945373535\n",
      "    Train population | accuracy after local training  : 0.14128\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 62006\n",
      "    Val population | number of devices                : 2\n",
      "    Val population | number of data points            : 1000.0\n",
      "    Val population | loss before local training       : 0.0023008668422698974\n",
      "    Val population | accuracy before local training   : 0.103\n",
      "    Learning rate                                     : 0.1\n",
      "    Central val | accuracy                            : 0.1005\n",
      "    Central val | loss                                : 4.5314526557922366e-05\n",
      "    Central val | number of data points               : 50000.0\n",
      "Metrics at iteration 2 ():\n",
      "    Train population | number of devices              : 50\n",
      "    Train population | number of data points          : 5000.0\n",
      "    Train population | loss before local training     : 0.00045316905403137205\n",
      "    Train population | accuracy before local training : 0.100124\n",
      "    Train population | loss after local training      : 0.0002952465224266052\n",
      "    Train population | accuracy after local training  : 0.143024\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 62006\n",
      "    Val population | number of devices                : 2\n",
      "    Val population | number of data points            : 1000.0\n",
      "    Val population | loss before local training       : 0.002269428610801697\n",
      "    Val population | accuracy before local training   : 0.102\n",
      "    Learning rate                                     : 0.1\n",
      "    Central val | accuracy                            : 0.12394\n",
      "    Central val | loss                                : 4.228061676025391e-05\n",
      "    Central val | number of data points               : 50000.0\n",
      "Metrics at iteration 3 ():\n",
      "    Train population | number of devices              : 50\n",
      "    Train population | number of data points          : 5000.0\n",
      "    Train population | loss before local training     : 0.00042268302726745607\n",
      "    Train population | accuracy before local training : 0.12506\n",
      "    Train population | loss after local training      : 0.0002653113465309143\n",
      "    Train population | accuracy after local training  : 0.144304\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 62006\n",
      "    Val population | number of devices                : 2\n",
      "    Val population | number of data points            : 1000.0\n",
      "    Val population | loss before local training       : 0.0021102335453033446\n",
      "    Val population | accuracy before local training   : 0.13\n",
      "    Learning rate                                     : 0.1\n",
      "    Central val | accuracy                            : 0.13784\n",
      "    Central val | loss                                : 3.4941949844360354e-05\n",
      "    Central val | number of data points               : 50000.0\n",
      "Metrics at iteration 4 ():\n",
      "    Train population | number of devices              : 50\n",
      "    Train population | number of data points          : 5000.0\n",
      "    Train population | loss before local training     : 0.00034923097610473633\n",
      "    Train population | accuracy before local training : 0.137004\n",
      "    Train population | loss after local training      : 0.0002440317063331604\n",
      "    Train population | accuracy after local training  : 0.153636\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 62006\n",
      "    Val population | number of devices                : 2\n",
      "    Val population | number of data points            : 1000.0\n",
      "    Val population | loss before local training       : 0.0017576528787612916\n",
      "    Val population | accuracy before local training   : 0.149\n",
      "    Learning rate                                     : 0.1\n",
      "    Central val | accuracy                            : 0.14404\n",
      "    Central val | loss                                : 2.8297388553619384e-05\n",
      "    Central val | number of data points               : 50000.0\n",
      "Metrics at iteration 5 ():\n",
      "    Train population | number of devices              : 50\n",
      "    Train population | number of data points          : 5000.0\n",
      "    Train population | loss before local training     : 0.0002835400333404541\n",
      "    Train population | accuracy before local training : 0.142816\n",
      "    Train population | loss after local training      : 0.00021752379894256592\n",
      "    Train population | accuracy after local training  : 0.154992\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 62006\n",
      "    Val population | number of devices                : 2\n",
      "    Val population | number of data points            : 1000.0\n",
      "    Val population | loss before local training       : 0.0014262927770614625\n",
      "    Val population | accuracy before local training   : 0.15\n",
      "    Learning rate                                     : 0.1\n",
      "    Central val | accuracy                            : 0.14706\n",
      "    Central val | loss                                : 3.146430969238281e-05\n",
      "    Central val | number of data points               : 50000.0\n",
      "Metrics at iteration 6 ():\n",
      "    Train population | number of devices              : 50\n",
      "    Train population | number of data points          : 5000.0\n",
      "    Train population | loss before local training     : 0.0003149823637008667\n",
      "    Train population | accuracy before local training : 0.147844\n",
      "    Train population | loss after local training      : 0.00020217993807792665\n",
      "    Train population | accuracy after local training  : 0.162904\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 62006\n",
      "    Val population | number of devices                : 2\n",
      "    Val population | number of data points            : 1000.0\n",
      "    Val population | loss before local training       : 0.001614816665649414\n",
      "    Val population | accuracy before local training   : 0.156\n",
      "    Learning rate                                     : 0.1\n",
      "    Central val | accuracy                            : 0.14858\n",
      "    Central val | loss                                : 4.31544828414917e-05\n",
      "    Central val | number of data points               : 50000.0\n",
      "Metrics at iteration 7 ():\n",
      "    Train population | number of devices              : 50\n",
      "    Train population | number of data points          : 5000.0\n",
      "    Train population | loss before local training     : 0.0004315247611999512\n",
      "    Train population | accuracy before local training : 0.149508\n",
      "    Train population | loss after local training      : 0.0001928540642261505\n",
      "    Train population | accuracy after local training  : 0.164304\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 62006\n",
      "    Val population | number of devices                : 2\n",
      "    Val population | number of data points            : 1000.0\n",
      "    Val population | loss before local training       : 0.0023748663663864136\n",
      "    Val population | accuracy before local training   : 0.149\n",
      "    Learning rate                                     : 0.1\n",
      "    Central val | accuracy                            : 0.14966\n",
      "    Central val | loss                                : 5.6234188079833985e-05\n",
      "    Central val | number of data points               : 50000.0\n",
      "Metrics at iteration 8 ():\n",
      "    Train population | number of devices              : 50\n",
      "    Train population | number of data points          : 5000.0\n",
      "    Train population | loss before local training     : 0.0005645416593551636\n",
      "    Train population | accuracy before local training : 0.150396\n",
      "    Train population | loss after local training      : 0.0001666726882457733\n",
      "    Train population | accuracy after local training  : 0.168808\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 62006\n",
      "    Val population | number of devices                : 2\n",
      "    Val population | number of data points            : 1000.0\n",
      "    Val population | loss before local training       : 0.0030716251134872436\n",
      "    Val population | accuracy before local training   : 0.1315\n",
      "    Learning rate                                     : 0.1\n",
      "    Central val | accuracy                            : 0.1515\n",
      "    Central val | loss                                : 6.381181716918946e-05\n",
      "    Central val | number of data points               : 50000.0\n",
      "Metrics at iteration 9 ():\n",
      "    Train population | number of devices              : 50\n",
      "    Train population | number of data points          : 5000.0\n",
      "    Train population | loss before local training     : 0.0006340786113739013\n",
      "    Train population | accuracy before local training : 0.150576\n",
      "    Train population | loss after local training      : 0.00015522917747497558\n",
      "    Train population | accuracy after local training  : 0.16814\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 62006\n",
      "    Val population | number of devices                : 2\n",
      "    Val population | number of data points            : 1000.0\n",
      "    Val population | loss before local training       : 0.0034899652004241944\n",
      "    Val population | accuracy before local training   : 0.1565\n",
      "    Learning rate                                     : 0.1\n",
      "    Central val | accuracy                            : 0.15274\n",
      "    Central val | loss                                : 6.301578044891358e-05\n",
      "    Central val | number of data points               : 50000.0\n"
     ]
    }
   ],
   "source": [
    "# PFL training using DP\n",
    "\n",
    "pfl_model = algorithm.run(\n",
    "    backend=pfl_simulated_backend,\n",
    "    model=model,\n",
    "    algorithm_params=algorithm_params,\n",
    "    model_train_params=model_train_params,\n",
    "    model_eval_params=model_eval_params,\n",
    "    callbacks=pfl_callbacks,\n",
    "    send_metrics_to_platform=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
