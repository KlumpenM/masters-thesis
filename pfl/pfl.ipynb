{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Both Jupyter and `pfl` use async. `nest_asyncio` allows `pfl` to run inside the notebook \n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# append the root directory to your paths to be able to reach the examples.  \n",
    "torch.random.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# Always import the `pfl` model first before initializing any `pfl` components to let `pfl` know which Deep Learning framework you will use.\n",
    "import multiprocessing\n",
    "# Set multiprocessing start method to \"spawn\" instead of forkserver (which is the default)\n",
    "# That is because forkserver does not work on Windows, but spawn does.\n",
    "def init_multiprocessing():\n",
    "    try:\n",
    "        multiprocessing.set_start_method(\"spawn\", force=True)  # Forces \"spawn\"\n",
    "    except RuntimeError:\n",
    "        pass  # Ignore if it's already set\n",
    "\n",
    "init_multiprocessing()\n",
    "\n",
    "from pfl.model.pytorch import PyTorchModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the DP mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Gaussian DP mechanisms for central DP guarantees using three different methods\n",
    "\n",
    "clipping_bound = 0.5\n",
    "epsilon = 2\n",
    "delta = 1e-8\n",
    "cohort_size = 2\n",
    "num_epochs = 100 # For DP\n",
    "central_num_iterations = 3\n",
    "sampling_probability = 1e-4\n",
    "is_central = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.gautamkamath.com/CS860notes/lec5.pdf\n",
    "\n",
    "Definition 5 on page 3 gives the definition of the parameters for Gaussian distribution where we sample noise for DP.\n",
    "\n",
    "$\\Delta_2^2$ is not included when defining `relative_noise_stddev`, because the clipping bound (which I understand to be $\\Delta_2^2$ and also is $\\Delta_2^{(f)}$ from definition 3 on page 3) is multiplied on `relative_noise_stddev`, when sampling the noise for DP via `GaussianMechanism.add_noise()`\n",
    "\n",
    "Source: https://apple.github.io/pfl-research/reference/privacy.html#pfl.privacy.gaussian_mechanism.GaussianMechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "relative_noise_stddev = 2 * math.log(1.25 / delta) * 1/(epsilon**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a Gaussian DP mechanism using the PLD privacy accountant\n",
    "# WARNING: it takes a while for the gaussian_moments_accountant mechanism to be instantiated\n",
    "\n",
    "from pfl.privacy import (PLDPrivacyAccountant, CentrallyAppliedPrivacyMechanism, GaussianMechanism, LocalPrivacyMechanism)\n",
    "\n",
    "# define the PLD privacy accountant, which will use the Gaussian noise mechanism\n",
    "pld_accountant = PLDPrivacyAccountant(\n",
    "    num_compositions=num_epochs,\n",
    "    sampling_probability=sampling_probability,\n",
    "    mechanism='gaussian',\n",
    "    epsilon=epsilon,\n",
    "    delta=delta)\n",
    "\n",
    "# instantiate a Gaussian noise mechanism using the privacy accountant\n",
    "pld_gaussian_noise_mechanism = GaussianMechanism.from_privacy_accountant(\n",
    "    accountant=pld_accountant, clipping_bound=clipping_bound)\n",
    "\n",
    "# wrap the noise mechanism with CentrallyAppliedPrivacyMechanism to make it a central privacy mechanism\n",
    "pld_central_privacy = CentrallyAppliedPrivacyMechanism(pld_gaussian_noise_mechanism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Local DP mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://apple.github.io/pfl-research/reference/privacy.html#pfl.privacy.gaussian_mechanism.GaussianMechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfl.metrics import Metrics\n",
    "\n",
    "class LocallyAppliedPrivacyMechanism(LocalPrivacyMechanism):\n",
    "\n",
    "    def privatize(self, statistics, name_formatting_fn=..., seed = None):\n",
    "        # TODO: Implement actual privatization for local DP\n",
    "        # TODO: Sample some noise (Gaussian: parameters depend on privacy parameters)\n",
    "        gaussian_mechanism = GaussianMechanism(clipping_bound=clipping_bound, \n",
    "                                               relative_noise_stddev=relative_noise_stddev)\n",
    "        noisy_statistics, metrics = gaussian_mechanism.add_noise(statistics=statistics, \n",
    "                                     cohort_size=cohort_size, \n",
    "                                     name_formatting_fn=name_formatting_fn)\n",
    "        # TODO: Add the noise to statistics then send it\n",
    "        return noisy_statistics, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, Optional\n",
    "from pfl.metrics import Weighted\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "pytorch_model = Net()\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def loss(inputs: torch.Tensor, targets: torch.Tensor, eval: bool = False) -> torch.Tensor:\n",
    "    pytorch_model.eval() if eval else pytorch_model.train()\n",
    "    return loss_fn(pytorch_model(inputs), targets)\n",
    "\n",
    "\"\"\"\n",
    "@torch.no_grad()\n",
    "def metrics(inputs: torch.Tensor,\n",
    "             targets: torch.Tensor,\n",
    "             eval: bool = True) -> Dict[str, Weighted]:\n",
    "    pytorch_model.eval() if eval else pytorch_model.train()\n",
    "    prediction = pytorch_model(inputs)\n",
    "    logits = torch.argmax(prediction, dim=1)\n",
    "    num_samples = len(inputs)\n",
    "    num_predictions = targets.numel()\n",
    "    correct = torch.sum(torch.eq((logits > 0.0).float(), targets))\n",
    "\n",
    "    loss = loss_fn(prediction, targets).item()\n",
    "    return {\n",
    "        \"loss\": Weighted(loss, num_samples),\n",
    "        \"accuracy\": Weighted(correct, num_predictions)\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def metrics(inputs: torch.Tensor,\n",
    "             targets: torch.Tensor,\n",
    "             eval: bool = True) -> Dict[str, Weighted]:\n",
    "    pytorch_model.eval() if eval else pytorch_model.train()\n",
    "    prediction = pytorch_model(inputs)\n",
    "    logits = torch.argmax(prediction, dim=1)\n",
    "    num_samples = len(inputs)\n",
    "    num_predictions = targets.numel()\n",
    "    correct = torch.sum(torch.eq((logits > 0.0).float(), targets))\n",
    "\n",
    "    loss = loss_fn(prediction, targets).item()\n",
    "    return {\n",
    "        \"loss\": Weighted(loss, num_samples),\n",
    "        \"accuracy\": Weighted(correct, num_samples)\n",
    "    }\n",
    "\n",
    "\n",
    "pytorch_model.loss = loss\n",
    "pytorch_model.metrics = metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfl.data.dataset import Dataset\n",
    "import sys\n",
    "\n",
    "sys.path.append('../flower/flower_normal')\n",
    "from centralized import load_data\n",
    "train_set, test_set = load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training dataloader into features and labels\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "for features, labels in train_set:\n",
    "    all_features.append(features)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "# Convert to tensors if needed\n",
    "all_features = torch.cat(all_features, dim=0)\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "central_data = Dataset([all_features, all_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 3, 32, 32])\n",
      "torch.Size([50000])\n"
     ]
    }
   ],
   "source": [
    "print(all_features.shape)\n",
    "print(all_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the validation dataloader into features and labels\n",
    "val_all_features = []\n",
    "val_all_labels = []\n",
    "\n",
    "for features, labels in test_set:\n",
    "    val_all_features.append(features)\n",
    "    val_all_labels.append(labels)\n",
    "\n",
    "# Convert to tensors if needed\n",
    "val_all_features = torch.cat(val_all_features, dim=0)\n",
    "val_all_labels = torch.cat(val_all_labels, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 3, 32, 32])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(val_all_features.shape)\n",
    "print(val_all_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "n_samples = len(train_set.dataset)\n",
    "print(n_samples)\n",
    "val_n_samples = len(test_set.dataset)\n",
    "print(val_n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfl.data import ArtificialFederatedDataset, get_data_sampler\n",
    "\n",
    "# Create data sampler to sample each artificial user dataset as a random subset of the original dataset\n",
    "data_sampler = get_data_sampler(sample_type=\"random\", max_bound=n_samples)\n",
    "\n",
    "# Create an artificial federated dataset where each user dataset has constant size such that there are 10 users to distribute among\n",
    "sample_dataset_len = lambda: int(n_samples/10)\n",
    "federated_dataset = ArtificialFederatedDataset.from_slices(\n",
    "    data=[all_features, all_labels], \n",
    "    data_sampler=data_sampler,\n",
    "    sample_dataset_len=sample_dataset_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_sampler = get_data_sampler(sample_type=\"random\", max_bound=val_n_samples)\n",
    "\n",
    "# Create an artificial federated dataset where each user dataset has constant size such that there are of 10 users to distribute among\n",
    "val_sample_dataset_len = lambda: int(val_n_samples/10)\n",
    "val_federated_dataset = ArtificialFederatedDataset.from_slices(\n",
    "    data=[val_all_features, val_all_labels],\n",
    "    data_sampler=val_data_sampler,\n",
    "    sample_dataset_len=val_sample_dataset_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in pytorch_model.parameters() if p.requires_grad]\n",
    "\n",
    "model = PyTorchModel(pytorch_model, \n",
    "                     local_optimizer_create=torch.optim.SGD,\n",
    "                     central_optimizer=torch.optim.SGD(params, 0.1, momentum=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataLoader' object has no attribute 'get_worker_partition'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 22\u001b[0m\n\u001b[0;32m     15\u001b[0m evaluation_frequency \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     16\u001b[0m algorithm_params \u001b[38;5;241m=\u001b[39m NNAlgorithmParams(\n\u001b[0;32m     17\u001b[0m     central_num_iterations\u001b[38;5;241m=\u001b[39mcentral_num_iterations,\n\u001b[0;32m     18\u001b[0m     evaluation_frequency\u001b[38;5;241m=\u001b[39mevaluation_frequency,\n\u001b[0;32m     19\u001b[0m     train_cohort_size\u001b[38;5;241m=\u001b[39mcohort_size,\n\u001b[0;32m     20\u001b[0m     val_cohort_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m pfl_callbacks \u001b[38;5;241m=\u001b[39m [\u001b[43mCentralEvaluationCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_eval_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluation_frequency\u001b[49m\u001b[43m)\u001b[49m, AggregateMetricsToDisk(output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpfl_training_metrics/metrics.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#postprocessors = [LocallyAppliedPrivacyMechanism()]\u001b[39;00m\n\u001b[0;32m     25\u001b[0m postprocessors \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\AVN\\anaconda3\\envs\\masters\\lib\\site-packages\\pfl\\callback.py:191\u001b[0m, in \u001b[0;36mCentralEvaluationCallback.__init__\u001b[1;34m(self, dataset, model_eval_params, frequency, distribute_evaluation, format_fn)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset \u001b[38;5;241m=\u001b[39m dataset\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_eval_params \u001b[38;5;241m=\u001b[39m model_eval_params\n\u001b[1;32m--> 191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_partial_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_worker_partition\u001b[49m()\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_frequency \u001b[38;5;241m=\u001b[39m frequency\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_evaluation \u001b[38;5;241m=\u001b[39m distribute_evaluation\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'get_worker_partition'"
     ]
    }
   ],
   "source": [
    "\n",
    "from pfl.algorithm import FederatedAveraging, NNAlgorithmParams\n",
    "from pfl.callback import CentralEvaluationCallback, AggregateMetricsToDisk\n",
    "from pfl.hyperparam import NNTrainHyperParams, NNEvalHyperParams\n",
    "from pfl.aggregate.simulate import SimulatedBackend\n",
    "\n",
    "\n",
    "model_train_params = NNTrainHyperParams(\n",
    "    local_learning_rate=0.1,\n",
    "    local_num_epochs=3,\n",
    "    local_batch_size=32)\n",
    "\n",
    "# Do full-batch evaluation to run faster.\n",
    "model_eval_params = NNEvalHyperParams(local_batch_size=None)\n",
    "\n",
    "evaluation_frequency = 1\n",
    "algorithm_params = NNAlgorithmParams(\n",
    "    central_num_iterations=central_num_iterations,\n",
    "    evaluation_frequency=evaluation_frequency,\n",
    "    train_cohort_size=cohort_size,\n",
    "    val_cohort_size=2)\n",
    "\n",
    "pfl_callbacks = [CentralEvaluationCallback(central_data, model_eval_params, evaluation_frequency), AggregateMetricsToDisk(output_path='pfl_training_metrics/metrics.csv')]\n",
    "\n",
    "#postprocessors = [LocallyAppliedPrivacyMechanism()]\n",
    "postprocessors = []\n",
    "\n",
    "pfl_simulated_backend = SimulatedBackend(\n",
    "    training_data=federated_dataset,\n",
    "    val_data=val_federated_dataset,\n",
    "    postprocessors=postprocessors\n",
    ")\n",
    "\n",
    "\n",
    "algorithm = FederatedAveraging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics at iteration 0 ():\n",
      "    Central val | loss                                : 4.6076517105102536e-05\n",
      "    Central val | accuracy                            : 0.1\n",
      "    Central val | number of data points               : 50000\n",
      "    Train population | number of devices              : 2\n",
      "    Train population | number of data points          : 5000.0\n",
      "    Train population | loss before local training     : 0.0004608012914657593\n",
      "    Train population | accuracy before local training : 0.1\n",
      "    Train population | loss after local training      : 0.00036396725177764895\n",
      "    Train population | accuracy after local training  : 0.147\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 62006\n",
      "    Val population | number of devices                : 2\n",
      "    Val population | number of data points            : 1000.0\n",
      "    Val population | loss before local training       : 0.0023041212558746337\n",
      "    Val population | accuracy before local training   : 0.1085\n",
      "    Learning rate                                     : 0.1\n",
      "Metrics at iteration 1 ():\n",
      "    Train population | number of devices              : 2\n",
      "    Train population | number of data points          : 5000.0\n",
      "    Train population | loss before local training     : 0.0004602432489395142\n",
      "    Train population | accuracy before local training : 0.1044\n",
      "    Train population | loss after local training      : 0.00035087482929229736\n",
      "    Train population | accuracy after local training  : 0.1373\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 62006\n",
      "    Val population | number of devices                : 2\n",
      "    Val population | number of data points            : 1000.0\n",
      "    Val population | loss before local training       : 0.0023013637065887453\n",
      "    Val population | accuracy before local training   : 0.103\n",
      "    Learning rate                                     : 0.1\n",
      "    Central val | accuracy                            : 0.10014\n",
      "    Central val | loss                                : 4.540508270263672e-05\n",
      "    Central val | number of data points               : 50000.0\n",
      "Metrics at iteration 2 ():\n",
      "    Train population | number of devices              : 2\n",
      "    Train population | number of data points          : 5000.0\n",
      "    Train population | loss before local training     : 0.0004545206785202026\n",
      "    Train population | accuracy before local training : 0.1027\n",
      "    Train population | loss after local training      : 0.0003868170738220215\n",
      "    Train population | accuracy after local training  : 0.1559\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 62006\n",
      "    Val population | number of devices                : 2\n",
      "    Val population | number of data points            : 1000.0\n",
      "    Val population | loss before local training       : 0.0022729274034500122\n",
      "    Val population | accuracy before local training   : 0.102\n",
      "    Learning rate                                     : 0.1\n",
      "    Central val | accuracy                            : 0.12848\n",
      "    Central val | loss                                : 4.279855251312256e-05\n",
      "    Central val | number of data points               : 50000.0\n"
     ]
    }
   ],
   "source": [
    "# PFL training using DP\n",
    "\n",
    "pfl_model = algorithm.run(\n",
    "    backend=pfl_simulated_backend,\n",
    "    model=model,\n",
    "    algorithm_params=algorithm_params,\n",
    "    model_train_params=model_train_params,\n",
    "    model_eval_params=model_eval_params,\n",
    "    callbacks=pfl_callbacks,\n",
    "    send_metrics_to_platform=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
