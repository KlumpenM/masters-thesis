{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Both Jupyter and `pfl` use async. `nest_asyncio` allows `pfl` to run inside the notebook \n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# append the root directory to your paths to be able to reach the examples.  \n",
    "torch.random.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# Always import the `pfl` model first before initializing any `pfl` components to let `pfl` know which Deep Learning framework you will use.\n",
    "import multiprocessing\n",
    "# Set multiprocessing start method to \"spawn\" instead of forkserver (which is the default)\n",
    "# That is because forkserver does not work on Windows, but spawn does.\n",
    "def init_multiprocessing():\n",
    "    try:\n",
    "        multiprocessing.set_start_method(\"spawn\", force=True)  # Forces \"spawn\"\n",
    "    except RuntimeError:\n",
    "        pass  # Ignore if it's already set\n",
    "\n",
    "init_multiprocessing()\n",
    "\n",
    "from pfl.model.pytorch import PyTorchModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the DP mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Gaussian DP mechanisms for central DP guarantees using three different methods\n",
    "\n",
    "clipping_bound = 0.5\n",
    "epsilon = 2\n",
    "delta = 1e-8\n",
    "cohort_size = 50\n",
    "num_epochs = 100 # For DP\n",
    "central_num_iterations = 10\n",
    "sampling_probability = 1e-4\n",
    "is_central = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.gautamkamath.com/CS860notes/lec5.pdf\n",
    "\n",
    "Definition 5 on page 3 gives the definition of the parameters for Gaussian distribution where we sample noise for DP.\n",
    "\n",
    "$\\Delta_2^2$ is not included when defining `relative_noise_stddev`, because the clipping bound (which I understand to be $\\Delta_2^2$ and also is $\\Delta_2^{(f)}$ from definition 3 on page 3) is multiplied on `relative_noise_stddev`, when sampling the noise for DP via `GaussianMechanism.add_noise()`\n",
    "\n",
    "Source: https://apple.github.io/pfl-research/reference/privacy.html#pfl.privacy.gaussian_mechanism.GaussianMechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "relative_noise_stddev = 2 * math.log(1.25 / delta) * 1/(epsilon**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a Gaussian DP mechanism using the PLD privacy accountant\n",
    "# WARNING: it takes a while for the gaussian_moments_accountant mechanism to be instantiated\n",
    "\n",
    "from pfl.privacy import (PLDPrivacyAccountant, CentrallyAppliedPrivacyMechanism, GaussianMechanism, LocalPrivacyMechanism)\n",
    "\n",
    "# define the PLD privacy accountant, which will use the Gaussian noise mechanism\n",
    "pld_accountant = PLDPrivacyAccountant(\n",
    "    num_compositions=num_epochs,\n",
    "    sampling_probability=sampling_probability,\n",
    "    mechanism='gaussian',\n",
    "    epsilon=epsilon,\n",
    "    delta=delta)\n",
    "\n",
    "# instantiate a Gaussian noise mechanism using the privacy accountant\n",
    "pld_gaussian_noise_mechanism = GaussianMechanism.from_privacy_accountant(\n",
    "    accountant=pld_accountant, clipping_bound=clipping_bound)\n",
    "\n",
    "# wrap the noise mechanism with CentrallyAppliedPrivacyMechanism to make it a central privacy mechanism\n",
    "pld_central_privacy = CentrallyAppliedPrivacyMechanism(pld_gaussian_noise_mechanism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Local DP mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://apple.github.io/pfl-research/reference/privacy.html#pfl.privacy.gaussian_mechanism.GaussianMechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfl.metrics import Metrics\n",
    "\n",
    "class LocallyAppliedPrivacyMechanism(LocalPrivacyMechanism):\n",
    "\n",
    "    def privatize(self, statistics, name_formatting_fn=..., seed = None):\n",
    "        # TODO: Implement actual privatization for local DP\n",
    "        # TODO: Sample some noise (Gaussian: parameters depend on privacy parameters)\n",
    "        gaussian_mechanism = GaussianMechanism(clipping_bound=clipping_bound, \n",
    "                                               relative_noise_stddev=relative_noise_stddev)\n",
    "        noisy_statistics, metrics = gaussian_mechanism.add_noise(statistics=statistics, \n",
    "                                     cohort_size=cohort_size, \n",
    "                                     name_formatting_fn=name_formatting_fn)\n",
    "        # TODO: Add the noise to statistics then send it\n",
    "        return noisy_statistics, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, Optional\n",
    "from pfl.metrics import Weighted\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "pytorch_model = Net()\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def loss(inputs: torch.Tensor, targets: torch.Tensor, eval: bool = False) -> torch.Tensor:\n",
    "    pytorch_model.eval() if eval else pytorch_model.train()\n",
    "    return loss_fn(pytorch_model(inputs), targets)\n",
    "\n",
    "\"\"\"\n",
    "@torch.no_grad()\n",
    "def metrics(inputs: torch.Tensor,\n",
    "             targets: torch.Tensor,\n",
    "             eval: bool = True) -> Dict[str, Weighted]:\n",
    "    pytorch_model.eval() if eval else pytorch_model.train()\n",
    "    prediction = pytorch_model(inputs)\n",
    "    logits = torch.argmax(prediction, dim=1)\n",
    "    num_samples = len(inputs)\n",
    "    num_predictions = targets.numel()\n",
    "    correct = torch.sum(torch.eq((logits > 0.0).float(), targets))\n",
    "\n",
    "    loss = loss_fn(prediction, targets).item()\n",
    "    return {\n",
    "        \"loss\": Weighted(loss, num_samples),\n",
    "        \"accuracy\": Weighted(correct, num_predictions)\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def metrics(inputs: torch.Tensor,\n",
    "             targets: torch.Tensor,\n",
    "             eval: bool = True) -> Dict[str, Weighted]:\n",
    "    pytorch_model.eval() if eval else pytorch_model.train()\n",
    "    prediction = pytorch_model(inputs)\n",
    "    logits = torch.argmax(prediction, dim=1)\n",
    "    num_samples = len(inputs)\n",
    "    num_predictions = targets.numel()\n",
    "    correct = torch.sum(torch.eq((logits > 0.0).float(), targets))\n",
    "\n",
    "    loss = loss_fn(prediction, targets).item()\n",
    "    return {\n",
    "        \"loss\": Weighted(loss, num_samples),\n",
    "        \"accuracy\": Weighted(correct, num_samples)\n",
    "    }\n",
    "\n",
    "\n",
    "pytorch_model.loss = loss\n",
    "pytorch_model.metrics = metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfl.data.dataset import Dataset\n",
    "import sys\n",
    "\n",
    "sys.path.append('../flower/flower_normal')\n",
    "from centralized import load_data\n",
    "train_set, test_set = load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training dataloader into features and labels\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "for features, labels in train_set:\n",
    "    all_features.append(features)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "# Convert to tensors if needed\n",
    "all_features = torch.cat(all_features, dim=0)\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "central_data = Dataset([all_features, all_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 3, 32, 32])\n",
      "torch.Size([50000])\n"
     ]
    }
   ],
   "source": [
    "print(all_features.shape)\n",
    "print(all_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the validation dataloader into features and labels\n",
    "val_all_features = []\n",
    "val_all_labels = []\n",
    "\n",
    "for features, labels in test_set:\n",
    "    val_all_features.append(features)\n",
    "    val_all_labels.append(labels)\n",
    "\n",
    "# Convert to tensors if needed\n",
    "val_all_features = torch.cat(val_all_features, dim=0)\n",
    "val_all_labels = torch.cat(val_all_labels, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 3, 32, 32])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(val_all_features.shape)\n",
    "print(val_all_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "n_samples = len(train_set.dataset)\n",
    "print(n_samples)\n",
    "val_n_samples = len(test_set.dataset)\n",
    "print(val_n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating artificial federated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfl.data import ArtificialFederatedDataset, get_data_sampler\n",
    "\n",
    "# Create data sampler to sample each artificial user dataset as a random subset of the original dataset\n",
    "data_sampler = get_data_sampler(sample_type=\"minimize_reuse\", max_bound=n_samples)\n",
    "\n",
    "# Create an artificial federated dataset where each user dataset has constant size such that there are 10 users to distribute among\n",
    "sample_dataset_len = lambda: int(n_samples/10)\n",
    "federated_dataset = ArtificialFederatedDataset.from_slices(\n",
    "    data=[all_features, all_labels], \n",
    "    data_sampler=data_sampler,\n",
    "    sample_dataset_len=sample_dataset_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_sampler = get_data_sampler(sample_type=\"minimize_reuse\", max_bound=val_n_samples)\n",
    "\n",
    "# Create an artificial federated dataset where each user dataset has constant size such that there are of 10 users to distribute among\n",
    "val_sample_dataset_len = lambda: int(val_n_samples/10)\n",
    "val_federated_dataset = ArtificialFederatedDataset.from_slices(\n",
    "    data=[val_all_features, val_all_labels],\n",
    "    data_sampler=val_data_sampler,\n",
    "    sample_dataset_len=val_sample_dataset_len\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a federated dataset\n",
    "\n",
    "For the purpose of being able to control the actual number of users, which `ArtificialFederatedDataset` does not allow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Reflection on designing `FederatedDataset`</strong>\n",
    "\n",
    "How many datapoints should each user have?\n",
    "- Every user has 50000/100 = 500 datapoints (easy approach)\n",
    "- Alternatively, do some randomization on how many datapoints each user has (harder approach)\n",
    "\n",
    "Should we use the exact same federated dataset in every FL experiment?\n",
    "- The datapoint will be allocated to the same user in every experiment. This means that user 0 will have the exact same datapoints. User 1 will also have the exact same datapoints and so on... for every single time we run our experiment. This means our federated dataset will be exactly the same every time we run our experiment.\n",
    "\n",
    "We decided to assume that all users have equally many datapoints and they will each possess the same datapoints every time we run our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfl.data import get_user_sampler, FederatedDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I must first decide how many users there are and then the method to how the data is distributed among the users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 3, 32, 32])\n",
      "torch.Size([500])\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter\n",
    "n_users = 100\n",
    "user_dataset_size = int(n_samples / n_users)\n",
    "print(user_dataset_size)\n",
    "\n",
    "# Maps user id to user dataset\n",
    "user_id_to_data = {}\n",
    "\n",
    "for i in range(n_users):\n",
    "    start = i*user_dataset_size\n",
    "    end = start+user_dataset_size\n",
    "    features = all_features[start:end]\n",
    "    print(features.shape)\n",
    "    labels = all_labels[start:end]\n",
    "    print(labels.shape)\n",
    "    user_id_to_data[i] = (features, labels)\n",
    "\n",
    "user_ids = list(user_id_to_data.keys())\n",
    "\n",
    "user_sampler = get_user_sampler(sample_type=\"random\", user_ids=user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_dataset = FederatedDataset.from_slices(\n",
    "    data=user_id_to_data,\n",
    "    user_sampler=user_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cohorts: <generator object FederatedDataset.get_cohort at 0x000001A83E5C7920>\n",
      "user_dataset: <pfl.data.dataset.Dataset object at 0x000001A81AFD6BC0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import pfl.data.dataset\n",
    "\n",
    "importlib.reload(pfl.data.dataset)\n",
    "\n",
    "from pfl.data.dataset import Dataset\n",
    "\n",
    "cohorts = federated_dataset.get_cohort(1)\n",
    "print(f'cohorts: {cohorts}')\n",
    "user_dataset, _ = next(cohorts)\n",
    "print(f'user_dataset: {user_dataset}')\n",
    "len(user_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pfl.internal.ops.selector import get_framework_module as get_ops\n",
    "from pfl.internal.ops.selector import has_framework_module\n",
    "\n",
    "def _first_tensor_length(data) -> int:\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return _first_tensor_length(data[0])\n",
    "    elif isinstance(data, dict):\n",
    "        return _first_tensor_length(next(iter(data.values())))\n",
    "    elif has_framework_module() and get_ops().is_tensor(data):\n",
    "        return get_ops().get_shape(data)[0]\n",
    "    else:\n",
    "        assert isinstance(data, np.ndarray), (\n",
    "            \"The data must be of type NumPy array. \"\n",
    "            \"If you want to use tensors of a particular framework you \"\n",
    "            \"must import its corresponding Model class from \"\n",
    "            \"`pfl.model` at the top of your script.\")\n",
    "        return data.shape[0]\n",
    "\n",
    "_first_tensor_length(all_features[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in pytorch_model.parameters() if p.requires_grad]\n",
    "\n",
    "model = PyTorchModel(pytorch_model, \n",
    "                     local_optimizer_create=torch.optim.SGD,\n",
    "                     central_optimizer=torch.optim.SGD(params, 0.1, momentum=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pfl.algorithm import FederatedAveraging, NNAlgorithmParams\n",
    "from pfl.callback import CentralEvaluationCallback, AggregateMetricsToDisk\n",
    "from pfl.hyperparam import NNTrainHyperParams, NNEvalHyperParams\n",
    "from pfl.aggregate.simulate import SimulatedBackend\n",
    "\n",
    "\n",
    "model_train_params = NNTrainHyperParams(\n",
    "    local_learning_rate=0.1,\n",
    "    local_num_epochs=5,\n",
    "    local_batch_size=32)\n",
    "\n",
    "# Do full-batch evaluation to run faster.\n",
    "model_eval_params = NNEvalHyperParams(local_batch_size=None)\n",
    "\n",
    "evaluation_frequency = 1\n",
    "algorithm_params = NNAlgorithmParams(\n",
    "    central_num_iterations=central_num_iterations,\n",
    "    evaluation_frequency=evaluation_frequency,\n",
    "    train_cohort_size=cohort_size,\n",
    "    val_cohort_size=0)\n",
    "\n",
    "pfl_callbacks = [CentralEvaluationCallback(central_data, model_eval_params, evaluation_frequency), AggregateMetricsToDisk(output_path='pfl_training_metrics/metrics.csv')]\n",
    "\n",
    "#postprocessors = [LocallyAppliedPrivacyMechanism()]\n",
    "postprocessors = []\n",
    "\n",
    "pfl_simulated_backend = SimulatedBackend(\n",
    "    training_data=federated_dataset,\n",
    "    val_data=None,\n",
    "    postprocessors=postprocessors\n",
    ")\n",
    "\n",
    "\n",
    "algorithm = FederatedAveraging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics at iteration 0 ():\n",
      "    Central val | loss                                : 4.6076517105102536e-05\n",
      "    Central val | accuracy                            : 0.1\n",
      "    Central val | number of data points               : 50000\n",
      "    Train population | number of devices              : 50\n",
      "    Train population | number of data points          : 500.0\n",
      "    Train population | loss before local training     : 0.004607966136932373\n",
      "    Train population | accuracy before local training : 0.10084\n",
      "    Train population | loss after local training      : 0.00453847583770752\n",
      "    Train population | accuracy after local training  : 0.10868\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 62006\n",
      "    Learning rate                                     : 0.1\n",
      "Metrics at iteration 1 ():\n",
      "    Train population | number of devices              : 50\n",
      "    Train population | number of data points          : 500.0\n",
      "    Train population | loss before local training     : 0.0046067279052734375\n",
      "    Train population | accuracy before local training : 0.10152\n",
      "    Train population | loss after local training      : 0.004451742963790893\n",
      "    Train population | accuracy after local training  : 0.11812\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 62006\n",
      "    Learning rate                                     : 0.1\n",
      "    Central val | accuracy                            : 0.1\n",
      "    Central val | loss                                : 4.601217269897461e-05\n",
      "    Central val | number of data points               : 50000.0\n",
      "Metrics at iteration 2 ():\n",
      "    Train population | number of devices              : 50\n",
      "    Train population | number of data points          : 500.0\n",
      "    Train population | loss before local training     : 0.00460163670539856\n",
      "    Train population | accuracy before local training : 0.09992\n",
      "    Train population | loss after local training      : 0.004274732313156128\n",
      "    Train population | accuracy after local training  : 0.12108\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 62006\n",
      "    Learning rate                                     : 0.1\n",
      "    Central val | accuracy                            : 0.1004\n",
      "    Central val | loss                                : 4.5824785232543945e-05\n",
      "    Central val | number of data points               : 50000.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# PFL training using DP\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m pfl_model \u001b[38;5;241m=\u001b[39m \u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpfl_simulated_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgorithm_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgorithm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_train_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_train_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_eval_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_eval_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpfl_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msend_metrics_to_platform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AVN\\anaconda3\\envs\\masters\\lib\\site-packages\\pfl\\algorithm\\base.py:286\u001b[0m, in \u001b[0;36mFederatedAlgorithm.run\u001b[1;34m(self, algorithm_params, backend, model, model_train_params, model_eval_params, callbacks, send_metrics_to_platform)\u001b[0m\n\u001b[0;32m    280\u001b[0m     all_metrics \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m on_train_metrics\n\u001b[0;32m    282\u001b[0m \u001b[38;5;66;03m# Step 2\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# Get aggregated model updates and\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# metrics from the requested queries.\u001b[39;00m\n\u001b[0;32m    285\u001b[0m results: List[Tuple[StatisticsType,\n\u001b[1;32m--> 286\u001b[0m                     Metrics]] \u001b[38;5;241m=\u001b[39m \u001b[43malgorithm_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_train_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcentral_contexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# Step 3\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# For each query result, accumulate metrics and\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;66;03m# let model handle statistics result if query had any.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m stats_context_pairs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\AVN\\anaconda3\\envs\\masters\\lib\\site-packages\\pfl\\algorithm\\algorithm_utils.py:63\u001b[0m, in \u001b[0;36mrun_train_eval\u001b[1;34m(algorithm, backend, model, central_contexts)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_all_iterations\u001b[39m():\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39m[\n\u001b[0;32m     59\u001b[0m         run_iteration(central_context, initial_sleep\u001b[38;5;241m=\u001b[39mindex \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m index, central_context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(central_contexts)\n\u001b[0;32m     61\u001b[0m     ])\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_event_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_all_iterations\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nest_asyncio.py:92\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     90\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nest_asyncio.py:133\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    130\u001b[0m curr_task \u001b[38;5;241m=\u001b[39m curr_tasks\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 133\u001b[0m     \u001b[43mhandle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;66;03m# restore the current task\u001b[39;00m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m curr_task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\AVN\\anaconda3\\envs\\masters\\lib\\asyncio\\events.py:80\u001b[0m, in \u001b[0;36mHandle._run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 80\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mSystemExit\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[0;32m     82\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\AVN\\anaconda3\\envs\\masters\\lib\\asyncio\\tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[1;32mc:\\Users\\AVN\\anaconda3\\envs\\masters\\lib\\site-packages\\pfl\\algorithm\\algorithm_utils.py:52\u001b[0m, in \u001b[0;36mrun_train_eval.<locals>.run_iteration\u001b[1;34m(central_context, initial_sleep)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m initial_sleep:\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# This is needed to ensure that the iteration with no sleep\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m# is ordered to start before other iterations with initial sleep.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# Which is needed for synchronous worker communication in\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# distributed simulations.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m backend\u001b[38;5;241m.\u001b[39masync_gather_results(\n\u001b[0;32m     53\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     54\u001b[0m     training_algorithm\u001b[38;5;241m=\u001b[39malgorithm,\n\u001b[0;32m     55\u001b[0m     central_context\u001b[38;5;241m=\u001b[39mcentral_context)\n",
      "File \u001b[1;32mc:\\Users\\AVN\\anaconda3\\envs\\masters\\lib\\site-packages\\pfl\\aggregate\\simulate.py:152\u001b[0m, in \u001b[0;36mSimulatedBackend.async_gather_results\u001b[1;34m(self, model, training_algorithm, central_context)\u001b[0m\n\u001b[0;32m    146\u001b[0m server_statistics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m user_dataset, local_seed \u001b[38;5;129;01min\u001b[39;00m selected_dataset\u001b[38;5;241m.\u001b[39mget_cohort(\n\u001b[0;32m    149\u001b[0m         cohort_size):\n\u001b[0;32m    151\u001b[0m     user_statistics, metrics_one_user \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 152\u001b[0m         \u001b[43mtraining_algorithm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulate_one_user\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mcentral_context\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    155\u001b[0m     user_context \u001b[38;5;241m=\u001b[39m UserContext(num_datapoints\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(user_dataset),\n\u001b[0;32m    156\u001b[0m                                seed\u001b[38;5;241m=\u001b[39mlocal_seed,\n\u001b[0;32m    157\u001b[0m                                user_id\u001b[38;5;241m=\u001b[39muser_dataset\u001b[38;5;241m.\u001b[39muser_id,\n\u001b[0;32m    158\u001b[0m                                metrics\u001b[38;5;241m=\u001b[39mmetrics_one_user)\n\u001b[0;32m    160\u001b[0m     num_total_datapoints \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m Weighted\u001b[38;5;241m.\u001b[39mfrom_unweighted(\n\u001b[0;32m    161\u001b[0m         user_context\u001b[38;5;241m.\u001b[39mnum_datapoints)\n",
      "File \u001b[1;32mc:\\Users\\AVN\\anaconda3\\envs\\masters\\lib\\site-packages\\pfl\\algorithm\\base.py:460\u001b[0m, in \u001b[0;36mFederatedNNAlgorithm.simulate_one_user\u001b[1;34m(self, model, user_dataset, central_context)\u001b[0m\n\u001b[0;32m    454\u001b[0m     metrics \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(user_dataset,\n\u001b[0;32m    455\u001b[0m                               initial_metrics_format_fn,\n\u001b[0;32m    456\u001b[0m                               central_context\u001b[38;5;241m.\u001b[39mmodel_eval_params)\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_model_state \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_parameters(\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_model_state)\n\u001b[1;32m--> 460\u001b[0m statistics, train_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_user\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initial_model_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcentral_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    463\u001b[0m metrics \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m train_metrics\n\u001b[0;32m    465\u001b[0m \u001b[38;5;66;03m# Evaluate after local training.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\AVN\\anaconda3\\envs\\masters\\lib\\site-packages\\pfl\\algorithm\\federated_averaging.py:45\u001b[0m, in \u001b[0;36mFederatedAveraging.train_one_user\u001b[1;34m(self, initial_model_state, model, user_dataset, central_context)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain_one_user\u001b[39m(\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m, initial_model_state: WeightedStatistics,\n\u001b[0;32m     41\u001b[0m     model: StatefulModelType, user_dataset: AbstractDatasetType,\n\u001b[0;32m     42\u001b[0m     central_context: FedAvgCentralContextType\n\u001b[0;32m     43\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[WeightedStatistics, Metrics]:\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# Local training loop\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     \u001b[43mbridges\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msgd_bridge\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_sgd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mcentral_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_train_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     training_statistics \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_model_difference(initial_model_state)\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# Don't reset model, will be used for evaluation after local training.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\AVN\\anaconda3\\envs\\masters\\lib\\site-packages\\pfl\\internal\\bridge\\pytorch\\sgd.py:41\u001b[0m, in \u001b[0;36mPyTorchSGDBridge.do_sgd\u001b[1;34m(model, user_dataset, train_params)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdo_sgd\u001b[39m(model: PyTorchModel, user_dataset: AbstractDatasetType,\n\u001b[0;32m     40\u001b[0m            train_params: NNTrainHyperParams) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 41\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_multiple_epochs_of\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m                                \u001b[49m\u001b[43m_sgd_train_step\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AVN\\anaconda3\\envs\\masters\\lib\\site-packages\\pfl\\model\\pytorch.py:369\u001b[0m, in \u001b[0;36mPyTorchModel.do_multiple_epochs_of\u001b[1;34m(self, user_dataset, train_params, train_step_fn, **kwargs)\u001b[0m\n\u001b[0;32m    367\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    368\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_batch(batch)\n\u001b[1;32m--> 369\u001b[0m         train_step_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model, local_optimizer, batch,\n\u001b[0;32m    370\u001b[0m                       user_dataset\u001b[38;5;241m.\u001b[39mtrain_kwargs, train_step_args,\n\u001b[0;32m    371\u001b[0m                       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LocalResultMetaData(num_steps\u001b[38;5;241m=\u001b[39mlocal_num_steps)\n",
      "File \u001b[1;32mc:\\Users\\AVN\\anaconda3\\envs\\masters\\lib\\site-packages\\pfl\\internal\\bridge\\pytorch\\sgd.py:24\u001b[0m, in \u001b[0;36m_sgd_train_step\u001b[1;34m(pytorch_model, local_optimizer, raw_data, train_kwargs, train_step_args)\u001b[0m\n\u001b[0;32m     21\u001b[0m     loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m train_step_args\u001b[38;5;241m.\u001b[39mgrad_accumulation_state\u001b[38;5;241m.\u001b[39maccumulation_steps\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_step_args\u001b[38;5;241m.\u001b[39mgrad_scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 24\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     train_step_args\u001b[38;5;241m.\u001b[39mgrad_scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\AVN\\anaconda3\\envs\\masters\\lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AVN\\anaconda3\\envs\\masters\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AVN\\anaconda3\\envs\\masters\\lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# PFL training using DP\n",
    "\n",
    "pfl_model = algorithm.run(\n",
    "    backend=pfl_simulated_backend,\n",
    "    model=model,\n",
    "    algorithm_params=algorithm_params,\n",
    "    model_train_params=model_train_params,\n",
    "    model_eval_params=model_eval_params,\n",
    "    callbacks=pfl_callbacks,\n",
    "    send_metrics_to_platform=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
