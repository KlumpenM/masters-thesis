{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Both Jupyter and `pfl` use async. `nest_asyncio` allows `pfl` to run inside the notebook \n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# append the root directory to your paths to be able to reach the examples.  \n",
    "torch.random.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# Always import the `pfl` model first before initializing any `pfl` components to let `pfl` know which Deep Learning framework you will use.\n",
    "import multiprocessing\n",
    "# Set multiprocessing start method to \"spawn\" instead of forkserver (which is the default)\n",
    "# That is because forkserver does not work on Windows, but spawn does.\n",
    "def init_multiprocessing():\n",
    "    try:\n",
    "        multiprocessing.set_start_method(\"spawn\", force=True)  # Forces \"spawn\"\n",
    "    except RuntimeError:\n",
    "        pass  # Ignore if it's already set\n",
    "\n",
    "init_multiprocessing()\n",
    "\n",
    "from pfl.model.pytorch import PyTorchModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the DP mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Gaussian DP mechanisms for central DP guarantees using three different methods\n",
    "\n",
    "clipping_bound = 0.5\n",
    "epsilon = 2\n",
    "delta = 1e-8\n",
    "cohort_size = 100\n",
    "num_epochs = 100\n",
    "sampling_probability = 1e-4\n",
    "is_central = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a Gaussian DP mechanism using the PLD privacy accountant\n",
    "# WARNING: it takes a while for the gaussian_moments_accountant mechanism to be instantiated\n",
    "\n",
    "from pfl.privacy import (PLDPrivacyAccountant, CentrallyAppliedPrivacyMechanism, GaussianMechanism)\n",
    "\n",
    "# define the PLD privacy accountant, which will use the Gaussian noise mechanism\n",
    "pld_accountant = PLDPrivacyAccountant(\n",
    "    num_compositions=num_epochs,\n",
    "    sampling_probability=sampling_probability,\n",
    "    mechanism='gaussian',\n",
    "    epsilon=epsilon,\n",
    "    delta=delta)\n",
    "\n",
    "# instantiate a Gaussian noise mechanism using the privacy accountant\n",
    "pld_gaussian_noise_mechanism = GaussianMechanism.from_privacy_accountant(\n",
    "    accountant=pld_accountant, clipping_bound=clipping_bound)\n",
    "\n",
    "# wrap the noise mechanism with CentrallyAppliedPrivacyMechanism to make it a central privacy mechanism\n",
    "pld_central_privacy = CentrallyAppliedPrivacyMechanism(pld_gaussian_noise_mechanism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, Optional\n",
    "from pfl.metrics import Weighted\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "pytorch_model = Net()\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def loss(inputs: torch.Tensor, targets: torch.Tensor, eval: bool = False) -> torch.Tensor:\n",
    "    pytorch_model.eval() if eval else pytorch_model.train()\n",
    "    return loss_fn(pytorch_model(inputs), targets)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def metrics(inputs: torch.Tensor,\n",
    "             targets: torch.Tensor,\n",
    "             eval: bool = True) -> Dict[str, Weighted]:\n",
    "    pytorch_model.eval() if eval else pytorch_model.train()\n",
    "    prediction = pytorch_model(inputs)\n",
    "    logits = torch.argmax(prediction, dim=1)\n",
    "    num_samples = len(inputs)\n",
    "    num_predictions = targets.numel()\n",
    "    correct = torch.sum(torch.eq((logits > 0.0).float(), targets))\n",
    "\n",
    "    loss = loss_fn(prediction, targets).item()\n",
    "    return {\n",
    "        \"loss\": Weighted(loss, num_samples),\n",
    "        \"accuracy\": Weighted(correct, num_predictions)\n",
    "    }\n",
    "\n",
    "pytorch_model.loss = loss\n",
    "pytorch_model.metrics = metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "from pfl.data.dataset import Dataset\n",
    "import sys\n",
    "\n",
    "sys.path.append('../flower/flower_normal')\n",
    "from centralized import load_data\n",
    "train_set, test_set = load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training dataloader into features and labels\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "for features, labels in train_set:\n",
    "    all_features.append(features)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "# Convert to tensors if needed\n",
    "all_features = torch.cat(all_features, dim=0)\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "\n",
    "central_num_iterations = 5\n",
    "central_data = Dataset([all_features, all_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 3, 32, 32])\n",
      "torch.Size([50000])\n"
     ]
    }
   ],
   "source": [
    "print(all_features.shape)\n",
    "print(all_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the validation dataloader into features and labels\n",
    "val_all_features = []\n",
    "val_all_labels = []\n",
    "\n",
    "for features, labels in test_set:\n",
    "    val_all_features.append(features)\n",
    "    val_all_labels.append(labels)\n",
    "\n",
    "# Convert to tensors if needed\n",
    "val_all_features = torch.cat(val_all_features, dim=0)\n",
    "val_all_labels = torch.cat(val_all_labels, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 3, 32, 32])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(val_all_features.shape)\n",
    "print(val_all_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "n_samples = len(train_set.dataset)\n",
    "print(n_samples)\n",
    "val_n_samples = len(test_set.dataset)\n",
    "print(val_n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfl.data import ArtificialFederatedDataset, get_data_sampler\n",
    "\n",
    "# Create data sampler to sample each artificial user dataset as a random subset of the original dataset\n",
    "data_sampler = get_data_sampler(sample_type=\"random\", max_bound=n_samples)\n",
    "\n",
    "# Create an artificial federated dataset where each user dataset has constant size of 10\n",
    "sample_dataset_len = lambda: 10\n",
    "federated_dataset = ArtificialFederatedDataset.from_slices(\n",
    "    data=[all_features, all_labels], \n",
    "    data_sampler=data_sampler,\n",
    "    sample_dataset_len=sample_dataset_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_sampler = get_data_sampler(sample_type=\"random\", max_bound=val_n_samples)\n",
    "\n",
    "# Create an artificial federated dataset where each user dataset has constant size of 10\n",
    "val_sample_dataset_len = lambda: 10\n",
    "val_federated_dataset = ArtificialFederatedDataset.from_slices(\n",
    "    data=[val_all_features, val_all_labels],\n",
    "    data_sampler=val_data_sampler,\n",
    "    sample_dataset_len=val_sample_dataset_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in pytorch_model.parameters() if p.requires_grad]\n",
    "\n",
    "model = PyTorchModel(pytorch_model, \n",
    "                     local_optimizer_create=torch.optim.SGD,\n",
    "                     central_optimizer=torch.optim.SGD(params, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PFL training: train with DP\n",
    "\n",
    "from pfl.algorithm import FederatedAveraging, NNAlgorithmParams\n",
    "from pfl.callback import CentralEvaluationCallback, AggregateMetricsToDisk\n",
    "from pfl.hyperparam import NNTrainHyperParams, NNEvalHyperParams\n",
    "from pfl.aggregate.simulate import SimulatedBackend\n",
    "\n",
    "\n",
    "model_train_params = NNTrainHyperParams(\n",
    "    local_learning_rate=0.05,\n",
    "    local_num_epochs=2,\n",
    "    local_batch_size=5)\n",
    "\n",
    "# Do full-batch evaluation to run faster.\n",
    "model_eval_params = NNEvalHyperParams(local_batch_size=None)\n",
    "\n",
    "evaluation_frequency = 5\n",
    "algorithm_params = NNAlgorithmParams(\n",
    "    central_num_iterations=central_num_iterations,\n",
    "    evaluation_frequency=evaluation_frequency,\n",
    "    train_cohort_size=cohort_size,\n",
    "    val_cohort_size=20)\n",
    "\n",
    "pfl_callbacks = [CentralEvaluationCallback(central_data, model_eval_params, evaluation_frequency), AggregateMetricsToDisk(output_path='pfl_training_metrics/metrics.csv')]\n",
    "\n",
    "postprocessors = [pld_central_privacy]\n",
    "\n",
    "pfl_simulated_backend = SimulatedBackend(\n",
    "    training_data=federated_dataset,\n",
    "    val_data=val_federated_dataset,\n",
    "    postprocessors=postprocessors\n",
    ")\n",
    "\n",
    "\n",
    "algorithm = FederatedAveraging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AVN\\anaconda3\\envs\\masters\\lib\\site-packages\\pfl\\internal\\ops\\pytorch_ops.py:342: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(v, device=get_default_device()).add(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics at iteration 0 ():\n",
      "    Central val | loss                                : 4.6076517105102536e-05\n",
      "    Central val | accuracy                            : 0.1\n",
      "    Central val | number of data points               : 50000\n",
      "    Train population | number of devices              : 100\n",
      "    Train population | number of data points          : 10.0\n",
      "    Train population | loss before local training     : 0.23036053657531738\n",
      "    Train population | accuracy before local training : 0.097\n",
      "    Train population | loss after local training      : 0.22754933476448058\n",
      "    Train population | accuracy after local training  : 0.097\n",
      "    Central DP | l2 norm bound                        : 0.5\n",
      "    Central DP | fraction of clipped norms            : 0.0\n",
      "    Central DP | norm before clipping                 : 0.07378956619650126\n",
      "    Train population | total weight                   : 1.0\n",
      "    Central DP | DP noise std. dev. on summed stats   : 0.24874210357666016\n",
      "    Central DP | signal-to-DP-noise ratio on summed stats: 0.012134754458177627\n",
      "    Number of parameters                              : 62006\n",
      "    Val population | number of devices                : 20\n",
      "    Val population | number of data points            : 10.0\n",
      "    Val population | loss before local training       : 0.23029194235801698\n",
      "    Val population | accuracy before local training   : 0.1\n",
      "    Learning rate                                     : 1.0\n",
      "Metrics at iteration 1 ():\n",
      "    Train population | number of devices              : 100\n",
      "    Train population | number of data points          : 10.0\n",
      "    Central DP | l2 norm bound                        : 0.5\n",
      "    Central DP | fraction of clipped norms            : 0.0\n",
      "    Central DP | norm before clipping                 : 0.07491695683449506\n",
      "    Train population | total weight                   : 1.0\n",
      "    Central DP | DP noise std. dev. on summed stats   : 0.24874210357666016\n",
      "    Central DP | signal-to-DP-noise ratio on summed stats: 0.01661414746297339\n",
      "    Number of parameters                              : 62006\n",
      "    Learning rate                                     : 1.0\n",
      "Metrics at iteration 2 ():\n",
      "    Train population | number of devices              : 100\n",
      "    Train population | number of data points          : 10.0\n",
      "    Central DP | l2 norm bound                        : 0.5\n",
      "    Central DP | fraction of clipped norms            : 0.0\n",
      "    Central DP | norm before clipping                 : 0.07550182685256004\n",
      "    Train population | total weight                   : 1.0\n",
      "    Central DP | DP noise std. dev. on summed stats   : 0.24874210357666016\n",
      "    Central DP | signal-to-DP-noise ratio on summed stats: 0.017548890783441934\n",
      "    Number of parameters                              : 62006\n",
      "    Learning rate                                     : 1.0\n",
      "Metrics at iteration 3 ():\n",
      "    Train population | number of devices              : 100\n",
      "    Train population | number of data points          : 10.0\n",
      "    Central DP | l2 norm bound                        : 0.5\n",
      "    Central DP | fraction of clipped norms            : 0.0\n",
      "    Central DP | norm before clipping                 : 0.07402165975421666\n",
      "    Train population | total weight                   : 1.0\n",
      "    Central DP | DP noise std. dev. on summed stats   : 0.24874210357666016\n",
      "    Central DP | signal-to-DP-noise ratio on summed stats: 0.015450574116845691\n",
      "    Number of parameters                              : 62006\n",
      "    Learning rate                                     : 1.0\n",
      "Metrics at iteration 4 ():\n",
      "    Train population | number of devices              : 100\n",
      "    Train population | number of data points          : 10.0\n",
      "    Central DP | l2 norm bound                        : 0.5\n",
      "    Central DP | fraction of clipped norms            : 0.0\n",
      "    Central DP | norm before clipping                 : 0.07618431061506271\n",
      "    Train population | total weight                   : 1.0\n",
      "    Central DP | DP noise std. dev. on summed stats   : 0.24874210357666016\n",
      "    Central DP | signal-to-DP-noise ratio on summed stats: 0.017533068520877906\n",
      "    Number of parameters                              : 62006\n",
      "    Learning rate                                     : 1.0\n"
     ]
    }
   ],
   "source": [
    "# PFL training using DP\n",
    "\n",
    "pfl_model = algorithm.run(\n",
    "    backend=pfl_simulated_backend,\n",
    "    model=model,\n",
    "    algorithm_params=algorithm_params,\n",
    "    model_train_params=model_train_params,\n",
    "    model_eval_params=model_eval_params,\n",
    "    callbacks=pfl_callbacks,\n",
    "    send_metrics_to_platform=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
