{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Both Jupyter and `pfl` use async. `nest_asyncio` allows `pfl` to run inside the notebook \n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# append the root directory to your paths to be able to reach the examples.  \n",
    "torch.random.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# Always import the `pfl` model first before initializing any `pfl` components to let `pfl` know which Deep Learning framework you will use.\n",
    "import multiprocessing\n",
    "# Set multiprocessing start method to \"spawn\" instead of forkserver (which is the default)\n",
    "# That is because forkserver does not work on Windows, but spawn does.\n",
    "def init_multiprocessing():\n",
    "    try:\n",
    "        multiprocessing.set_start_method(\"spawn\", force=True)  # Forces \"spawn\"\n",
    "    except RuntimeError:\n",
    "        pass  # Ignore if it's already set\n",
    "\n",
    "init_multiprocessing()\n",
    "\n",
    "from pfl.model.pytorch import PyTorchModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the DP mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Gaussian DP mechanisms for central DP guarantees using three different methods\n",
    "\n",
    "clipping_bound = 0.5\n",
    "epsilon = 2\n",
    "delta = 1e-8\n",
    "cohort_size = 2\n",
    "num_epochs = 100 # For DP\n",
    "central_num_iterations = 10\n",
    "sampling_probability = 1e-4\n",
    "is_central = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.gautamkamath.com/CS860notes/lec5.pdf\n",
    "\n",
    "Definition 5 on page 3 gives the definition of the parameters for Gaussian distribution where we sample noise for DP.\n",
    "\n",
    "$\\Delta_2^2$ is not included when defining `relative_noise_stddev`, because the clipping bound (which I understand to be $\\Delta_2^2$ and also is $\\Delta_2^{(f)}$ from definition 3 on page 3) is multiplied on `relative_noise_stddev`, when sampling the noise for DP via `GaussianMechanism.add_noise()`\n",
    "\n",
    "Source: https://apple.github.io/pfl-research/reference/privacy.html#pfl.privacy.gaussian_mechanism.GaussianMechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "relative_noise_stddev = 2 * math.log(1.25 / delta) * 1/(epsilon**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a Gaussian DP mechanism using the PLD privacy accountant\n",
    "# WARNING: it takes a while for the gaussian_moments_accountant mechanism to be instantiated\n",
    "\n",
    "from pfl.privacy import (PLDPrivacyAccountant, CentrallyAppliedPrivacyMechanism, GaussianMechanism, LocalPrivacyMechanism)\n",
    "\n",
    "# define the PLD privacy accountant, which will use the Gaussian noise mechanism\n",
    "pld_accountant = PLDPrivacyAccountant(\n",
    "    num_compositions=num_epochs,\n",
    "    sampling_probability=sampling_probability,\n",
    "    mechanism='gaussian',\n",
    "    epsilon=epsilon,\n",
    "    delta=delta)\n",
    "\n",
    "# instantiate a Gaussian noise mechanism using the privacy accountant\n",
    "pld_gaussian_noise_mechanism = GaussianMechanism.from_privacy_accountant(\n",
    "    accountant=pld_accountant, clipping_bound=clipping_bound)\n",
    "\n",
    "# wrap the noise mechanism with CentrallyAppliedPrivacyMechanism to make it a central privacy mechanism\n",
    "pld_central_privacy = CentrallyAppliedPrivacyMechanism(pld_gaussian_noise_mechanism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Local DP mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://apple.github.io/pfl-research/reference/privacy.html#pfl.privacy.gaussian_mechanism.GaussianMechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfl.metrics import Metrics\n",
    "\n",
    "class LocallyAppliedPrivacyMechanism(LocalPrivacyMechanism):\n",
    "\n",
    "    def privatize(self, statistics, name_formatting_fn=..., seed = None):\n",
    "        # TODO: Implement actual privatization for local DP\n",
    "        # TODO: Sample some noise (Gaussian: parameters depend on privacy parameters)\n",
    "        gaussian_mechanism = GaussianMechanism(clipping_bound=clipping_bound, \n",
    "                                               relative_noise_stddev=relative_noise_stddev)\n",
    "        noisy_statistics, metrics = gaussian_mechanism.add_noise(statistics=statistics, \n",
    "                                     cohort_size=cohort_size, \n",
    "                                     name_formatting_fn=name_formatting_fn)\n",
    "        # TODO: Add the noise to statistics then send it\n",
    "        return noisy_statistics, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Dict, Optional\n",
    "from pfl.metrics import Weighted\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "pytorch_model = Net()\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def loss(inputs: torch.Tensor, targets: torch.Tensor, eval: bool = False) -> torch.Tensor:\n",
    "    pytorch_model.eval() if eval else pytorch_model.train()\n",
    "    return loss_fn(pytorch_model(inputs), targets)\n",
    "\n",
    "\"\"\"\n",
    "@torch.no_grad()\n",
    "def metrics(inputs: torch.Tensor,\n",
    "             targets: torch.Tensor,\n",
    "             eval: bool = True) -> Dict[str, Weighted]:\n",
    "    pytorch_model.eval() if eval else pytorch_model.train()\n",
    "    prediction = pytorch_model(inputs)\n",
    "    logits = torch.argmax(prediction, dim=1)\n",
    "    num_samples = len(inputs)\n",
    "    num_predictions = targets.numel()\n",
    "    correct = torch.sum(torch.eq((logits > 0.0).float(), targets))\n",
    "\n",
    "    loss = loss_fn(prediction, targets).item()\n",
    "    return {\n",
    "        \"loss\": Weighted(loss, num_samples),\n",
    "        \"accuracy\": Weighted(correct, num_predictions)\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def metrics(inputs: torch.Tensor,\n",
    "             targets: torch.Tensor,\n",
    "             eval: bool = True) -> Dict[str, Weighted]:\n",
    "    pytorch_model.eval() if eval else pytorch_model.train()\n",
    "    prediction = pytorch_model(inputs)\n",
    "    logits = torch.argmax(prediction, dim=1)\n",
    "    num_samples = len(inputs)\n",
    "    num_predictions = targets.numel()\n",
    "    correct = torch.sum(torch.eq((logits > 0.0).float(), targets))\n",
    "\n",
    "    loss = loss_fn(prediction, targets).item()\n",
    "    return {\n",
    "        \"loss\": Weighted(loss, num_samples),\n",
    "        \"accuracy\": Weighted(correct, num_samples)\n",
    "    }\n",
    "\n",
    "\n",
    "pytorch_model.loss = loss\n",
    "pytorch_model.metrics = metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfl.data.dataset import Dataset\n",
    "import sys\n",
    "\n",
    "sys.path.append('../flower/flower_normal')\n",
    "from centralized import load_data\n",
    "train_set, test_set = load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training dataloader into features and labels\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "for features, labels in train_set:\n",
    "    all_features.append(features)\n",
    "    all_labels.append(labels)\n",
    "\n",
    "# Convert to tensors if needed\n",
    "all_features = torch.cat(all_features, dim=0)\n",
    "all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "central_data = Dataset([all_features, all_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 3, 32, 32])\n",
      "torch.Size([50000])\n"
     ]
    }
   ],
   "source": [
    "print(all_features.shape)\n",
    "print(all_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the validation dataloader into features and labels\n",
    "val_all_features = []\n",
    "val_all_labels = []\n",
    "\n",
    "for features, labels in test_set:\n",
    "    val_all_features.append(features)\n",
    "    val_all_labels.append(labels)\n",
    "\n",
    "# Convert to tensors if needed\n",
    "val_all_features = torch.cat(val_all_features, dim=0)\n",
    "val_all_labels = torch.cat(val_all_labels, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 3, 32, 32])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(val_all_features.shape)\n",
    "print(val_all_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "n_samples = len(train_set.dataset)\n",
    "print(n_samples)\n",
    "val_n_samples = len(test_set.dataset)\n",
    "print(val_n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfl.data import ArtificialFederatedDataset, get_data_sampler\n",
    "\n",
    "# Create data sampler to sample each artificial user dataset as a random subset of the original dataset\n",
    "data_sampler = get_data_sampler(sample_type=\"random\", max_bound=n_samples)\n",
    "\n",
    "# Create an artificial federated dataset where each user dataset has constant size such that there are 10 users to distribute among\n",
    "sample_dataset_len = lambda: int(n_samples/10)\n",
    "federated_dataset = ArtificialFederatedDataset.from_slices(\n",
    "    data=[all_features, all_labels], \n",
    "    data_sampler=data_sampler,\n",
    "    sample_dataset_len=sample_dataset_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_sampler = get_data_sampler(sample_type=\"random\", max_bound=val_n_samples)\n",
    "\n",
    "# Create an artificial federated dataset where each user dataset has constant size such that there are of 10 users to distribute among\n",
    "val_sample_dataset_len = lambda: int(val_n_samples/10)\n",
    "val_federated_dataset = ArtificialFederatedDataset.from_slices(\n",
    "    data=[val_all_features, val_all_labels],\n",
    "    data_sampler=val_data_sampler,\n",
    "    sample_dataset_len=val_sample_dataset_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in pytorch_model.parameters() if p.requires_grad]\n",
    "\n",
    "model = PyTorchModel(pytorch_model, \n",
    "                     local_optimizer_create=torch.optim.SGD,\n",
    "                     central_optimizer=torch.optim.SGD(params, 0.1, momentum=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pfl.algorithm import FederatedAveraging, NNAlgorithmParams\n",
    "from pfl.callback import CentralEvaluationCallback, AggregateMetricsToDisk\n",
    "from pfl.hyperparam import NNTrainHyperParams, NNEvalHyperParams\n",
    "from pfl.aggregate.simulate import SimulatedBackend\n",
    "\n",
    "\n",
    "model_train_params = NNTrainHyperParams(\n",
    "    local_learning_rate=0.1,\n",
    "    local_num_epochs=5,\n",
    "    local_batch_size=32)\n",
    "\n",
    "# Do full-batch evaluation to run faster.\n",
    "model_eval_params = NNEvalHyperParams(local_batch_size=None)\n",
    "\n",
    "evaluation_frequency = 1\n",
    "algorithm_params = NNAlgorithmParams(\n",
    "    central_num_iterations=central_num_iterations,\n",
    "    evaluation_frequency=evaluation_frequency,\n",
    "    train_cohort_size=cohort_size,\n",
    "    val_cohort_size=2)\n",
    "\n",
    "pfl_callbacks = [CentralEvaluationCallback(central_data, model_eval_params, evaluation_frequency), AggregateMetricsToDisk(output_path='pfl_training_metrics/metrics.csv')]\n",
    "\n",
    "#postprocessors = [LocallyAppliedPrivacyMechanism()]\n",
    "postprocessors = []\n",
    "\n",
    "pfl_simulated_backend = SimulatedBackend(\n",
    "    training_data=federated_dataset,\n",
    "    val_data=val_federated_dataset,\n",
    "    postprocessors=postprocessors\n",
    ")\n",
    "\n",
    "\n",
    "algorithm = FederatedAveraging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics at iteration 0 ():\n",
      "    Central val | loss                                : 4.6076517105102536e-05\n",
      "    Central val | accuracy                            : 0.1\n",
      "    Central val | number of data points               : 50000\n",
      "    Train population | number of devices              : 2\n",
      "    Train population | number of data points          : 5000.0\n",
      "    Train population | loss before local training     : 0.0004608012914657593\n",
      "    Train population | accuracy before local training : 0.1\n",
      "    Train population | loss after local training      : 0.0003381857514381409\n",
      "    Train population | accuracy after local training  : 0.1455\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 62006\n",
      "    Val population | number of devices                : 2\n",
      "    Val population | number of data points            : 1000.0\n",
      "    Val population | loss before local training       : 0.0023041212558746337\n",
      "    Val population | accuracy before local training   : 0.1085\n",
      "    Learning rate                                     : 0.1\n",
      "Metrics at iteration 1 ():\n",
      "    Train population | number of devices              : 2\n",
      "    Train population | number of data points          : 5000.0\n",
      "    Train population | loss before local training     : 0.00046018002033233643\n",
      "    Train population | accuracy before local training : 0.1044\n",
      "    Train population | loss after local training      : 0.00029407336711883544\n",
      "    Train population | accuracy after local training  : 0.1337\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 62006\n",
      "    Val population | number of devices                : 2\n",
      "    Val population | number of data points            : 1000.0\n",
      "    Val population | loss before local training       : 0.0023007134199142455\n",
      "    Val population | accuracy before local training   : 0.103\n",
      "    Learning rate                                     : 0.1\n",
      "    Central val | accuracy                            : 0.10004\n",
      "    Central val | loss                                : 4.532082557678222e-05\n",
      "    Central val | number of data points               : 50000.0\n",
      "Metrics at iteration 2 ():\n",
      "    Train population | number of devices              : 2\n",
      "    Train population | number of data points          : 5000.0\n",
      "    Train population | loss before local training     : 0.0004538691520690918\n",
      "    Train population | accuracy before local training : 0.1026\n",
      "    Train population | loss after local training      : 0.0003058178186416626\n",
      "    Train population | accuracy after local training  : 0.1675\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 62006\n",
      "    Val population | number of devices                : 2\n",
      "    Val population | number of data points            : 1000.0\n",
      "    Val population | loss before local training       : 0.002270121693611145\n",
      "    Val population | accuracy before local training   : 0.102\n",
      "    Learning rate                                     : 0.1\n",
      "    Central val | accuracy                            : 0.11762\n",
      "    Central val | loss                                : 4.218073844909668e-05\n",
      "    Central val | number of data points               : 50000.0\n",
      "Metrics at iteration 3 ():\n",
      "    Train population | number of devices              : 2\n",
      "    Train population | number of data points          : 5000.0\n",
      "    Train population | loss before local training     : 0.0004218372821807861\n",
      "    Train population | accuracy before local training : 0.121\n",
      "    Train population | loss after local training      : 0.0002348250150680542\n",
      "    Train population | accuracy after local training  : 0.1576\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 62006\n",
      "    Val population | number of devices                : 2\n",
      "    Val population | number of data points            : 1000.0\n",
      "    Val population | loss before local training       : 0.002104564428329468\n",
      "    Val population | accuracy before local training   : 0.12\n",
      "    Learning rate                                     : 0.1\n",
      "    Central val | accuracy                            : 0.13684\n",
      "    Central val | loss                                : 3.4515790939331056e-05\n",
      "    Central val | number of data points               : 50000.0\n",
      "Metrics at iteration 4 ():\n",
      "    Train population | number of devices              : 2\n",
      "    Train population | number of data points          : 5000.0\n",
      "    Train population | loss before local training     : 0.00034502063989639284\n",
      "    Train population | accuracy before local training : 0.1358\n",
      "    Train population | loss after local training      : 0.0002326008200645447\n",
      "    Train population | accuracy after local training  : 0.1632\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 62006\n",
      "    Val population | number of devices                : 2\n",
      "    Val population | number of data points            : 1000.0\n",
      "    Val population | loss before local training       : 0.001738337516784668\n",
      "    Val population | accuracy before local training   : 0.1485\n",
      "    Learning rate                                     : 0.1\n",
      "    Central val | accuracy                            : 0.14578\n",
      "    Central val | loss                                : 2.85028338432312e-05\n",
      "    Central val | number of data points               : 50000.0\n",
      "Metrics at iteration 5 ():\n",
      "    Train population | number of devices              : 2\n",
      "    Train population | number of data points          : 5000.0\n",
      "    Train population | loss before local training     : 0.0002851118087768555\n",
      "    Train population | accuracy before local training : 0.1451\n",
      "    Train population | loss after local training      : 0.00021811331510543822\n",
      "    Train population | accuracy after local training  : 0.163\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 62006\n",
      "    Val population | number of devices                : 2\n",
      "    Val population | number of data points            : 1000.0\n",
      "    Val population | loss before local training       : 0.0014352928996086121\n",
      "    Val population | accuracy before local training   : 0.1505\n",
      "    Learning rate                                     : 0.1\n",
      "    Central val | accuracy                            : 0.1501\n",
      "    Central val | loss                                : 3.375268459320068e-05\n",
      "    Central val | number of data points               : 50000.0\n",
      "Metrics at iteration 6 ():\n",
      "    Train population | number of devices              : 2\n",
      "    Train population | number of data points          : 5000.0\n",
      "    Train population | loss before local training     : 0.0003394479751586914\n",
      "    Train population | accuracy before local training : 0.1519\n",
      "    Train population | loss after local training      : 0.0001864306092262268\n",
      "    Train population | accuracy after local training  : 0.1609\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 62006\n",
      "    Val population | number of devices                : 2\n",
      "    Val population | number of data points            : 1000.0\n",
      "    Val population | loss before local training       : 0.0017429152131080627\n",
      "    Val population | accuracy before local training   : 0.162\n",
      "    Learning rate                                     : 0.1\n",
      "    Central val | accuracy                            : 0.1512\n",
      "    Central val | loss                                : 4.78171443939209e-05\n",
      "    Central val | number of data points               : 50000.0\n",
      "Metrics at iteration 7 ():\n",
      "    Train population | number of devices              : 2\n",
      "    Train population | number of data points          : 5000.0\n",
      "    Train population | loss before local training     : 0.00047744507789611817\n",
      "    Train population | accuracy before local training : 0.153\n",
      "    Train population | loss after local training      : 0.00017590395212173463\n",
      "    Train population | accuracy after local training  : 0.1737\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 62006\n",
      "    Val population | number of devices                : 2\n",
      "    Val population | number of data points            : 1000.0\n",
      "    Val population | loss before local training       : 0.0025892224311828615\n",
      "    Val population | accuracy before local training   : 0.152\n",
      "    Learning rate                                     : 0.1\n",
      "    Central val | accuracy                            : 0.15286\n",
      "    Central val | loss                                : 6.249572277069092e-05\n",
      "    Central val | number of data points               : 50000.0\n",
      "Metrics at iteration 8 ():\n",
      "    Train population | number of devices              : 2\n",
      "    Train population | number of data points          : 5000.0\n",
      "    Train population | loss before local training     : 0.0006168653249740601\n",
      "    Train population | accuracy before local training : 0.1538\n",
      "    Train population | loss after local training      : 0.00015314570069313048\n",
      "    Train population | accuracy after local training  : 0.1756\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 62006\n",
      "    Val population | number of devices                : 2\n",
      "    Val population | number of data points            : 1000.0\n",
      "    Val population | loss before local training       : 0.0034199279546737672\n",
      "    Val population | accuracy before local training   : 0.1335\n",
      "    Learning rate                                     : 0.1\n",
      "    Central val | accuracy                            : 0.1548\n",
      "    Central val | loss                                : 7.048846244812011e-05\n",
      "    Central val | number of data points               : 50000.0\n",
      "Metrics at iteration 9 ():\n",
      "    Train population | number of devices              : 2\n",
      "    Train population | number of data points          : 5000.0\n",
      "    Train population | loss before local training     : 0.0007198808193206787\n",
      "    Train population | accuracy before local training : 0.1539\n",
      "    Train population | loss after local training      : 0.0001429839313030243\n",
      "    Train population | accuracy after local training  : 0.1613\n",
      "    Train population | total weight                   : 1.0\n",
      "    Number of parameters                              : 62006\n",
      "    Val population | number of devices                : 2\n",
      "    Val population | number of data points            : 1000.0\n",
      "    Val population | loss before local training       : 0.0037553844451904297\n",
      "    Val population | accuracy before local training   : 0.166\n",
      "    Learning rate                                     : 0.1\n",
      "    Central val | accuracy                            : 0.15528\n",
      "    Central val | loss                                : 6.958388328552246e-05\n",
      "    Central val | number of data points               : 50000.0\n"
     ]
    }
   ],
   "source": [
    "# PFL training using DP\n",
    "\n",
    "pfl_model = algorithm.run(\n",
    "    backend=pfl_simulated_backend,\n",
    "    model=model,\n",
    "    algorithm_params=algorithm_params,\n",
    "    model_train_params=model_train_params,\n",
    "    model_eval_params=model_eval_params,\n",
    "    callbacks=pfl_callbacks,\n",
    "    send_metrics_to_platform=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
